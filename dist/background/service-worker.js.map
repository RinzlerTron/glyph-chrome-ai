{"version":3,"file":"background/service-worker.js","mappings":";;;;AAAA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;;AC5JA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AAEA;AACA;AACA;AAGA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;;ACzNA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AACA;AAAA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AAEA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAGA;AACA;AACA;AACA;AAIA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;;AClTA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;AAGA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;AAGA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AACA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AAMA;AAEA;AACA;AACA;AACA;AACA;AACA;AAGA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA;AAEA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjLA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAQA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AAIA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAKA;AACA;AAKA;AACA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC/RA;;AAEA;AACA;AAEA;AAIA;AAEA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AASA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AAIA;AAEA;AACA;AACA;AAKA;AACA;AACA;AACA;AAEA;AAIA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACxFA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AAEA;AAGA;AAAA;AAGA;AAEA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AAOA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AASA;AACA;AACA;AACA;AAaA;AACA;AAKA;AACA;AAWA;AACA;AAIA;AACA;AAIA;AACA;AACA;AACA;AAIA;AACA;AAKA;AAEA;AAEA;AACA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAGA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;;ACzUA;AACA;AACA;AACA;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AChCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC5BA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtGA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AACA;AAAA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AAEA;;AAOA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAQA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAKA;AACA;AACA;AACA;;AAEA;AAAA;AAEA;AAIA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAGA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;ACraA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AAEA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAGA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AAIA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAAA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAGA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;;ACvQA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AAGA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAQA;;AAIA;AACA;AACA;;AAIA;AACA;AAGA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;AAEA;AACA;;AC3KA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAGA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AAGA;AACA;AAAA;AAAA;AAAA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAGA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AAEA;;AAEA;AACA;AACA;AAKA;;AAEA;AACA;AACA;AAIA;AACA;AAGA;;AAEA;;AAEA;AACA;AACA;AAEA;AACA;AAMA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAAA;AAAA;AAEA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AAGA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;AAEA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;AAEA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAEA;AAEA;AACA;AACA;AAGA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AAAA;AAAA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAAA;AAAA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;AACA;AAEA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AASA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;AAGA;AACA;AACA;AACA;AAAA;AAAA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAEA;AAEA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AAMA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AAEA;AACA;AAEA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AAAA;AAAA;AAAA;AACA;AAEA;AAEA;AACA;AACA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AAEA;AAIA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AAIA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AACA;AAEA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AAKA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;AACA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AAAA;AACA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AAEA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AAAA;;AAEA;AACA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AAEA;AAAA;AAAA;AAEA;AAAA;AAAA;AACA;AACA;AACA;AAAA;AAAA;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AACA;AAAA;AAAA;AAAA;AACA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;;AAEA;AACA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AACA;AAAA;AAAA;AAAA;AAAA;AACA;AAGA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AAIA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;;AAEA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AAAA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AAEA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://glyph/./src/storage/db.js","webpack://glyph/./src/storage/graph-store.js","webpack://glyph/./src/graph/graph-engine.js","webpack://glyph/./src/utils/availability.js","webpack://glyph/./src/utils/constants.js","webpack://glyph/./src/utils/ai-helpers.js","webpack://glyph/./src/utils/text-processing.js","webpack://glyph/./src/graph/entity-extractor.js","webpack://glyph/./node_modules/uuid/dist/esm-browser/native.js","webpack://glyph/./node_modules/uuid/dist/esm-browser/rng.js","webpack://glyph/./node_modules/uuid/dist/esm-browser/stringify.js","webpack://glyph/./node_modules/uuid/dist/esm-browser/v4.js","webpack://glyph/./src/utils/helpers.js","webpack://glyph/./src/graph/relationship-finder.js","webpack://glyph/./src/utils/mock-data.js","webpack://glyph/./src/utils/writer-test.js","webpack://glyph/./src/background/service-worker.js"],"sourcesContent":["// IndexedDB wrapper for Glyph knowledge graph storage\nconst DB_NAME = 'GlyphDB';\nconst DB_VERSION = 2;\n\nclass Database {\n  constructor() {\n    this.db = null;\n  }\n\n  async init() {\n    if (this.db) return this.db;\n\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open(DB_NAME, DB_VERSION);\n\n      request.onerror = () => {\n        console.error('IndexedDB failed to open:', request.error);\n        reject(request.error);\n      };\n\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve(this.db);\n      };\n\n      request.onupgradeneeded = (event) => {\n        const db = event.target.result;\n\n        // Object Store: entities (knowledge graph nodes)\n        if (!db.objectStoreNames.contains('entities')) {\n          const entityStore = db.createObjectStore('entities', { keyPath: 'id' });\n          entityStore.createIndex('name', 'name', { unique: false });\n          entityStore.createIndex('type', 'type', { unique: false });\n          entityStore.createIndex('topic', 'topic', { unique: false });\n          entityStore.createIndex('firstSeen', 'firstSeen', { unique: false });\n        }\n\n        // Object Store: relationships (knowledge graph edges)\n        if (!db.objectStoreNames.contains('relationships')) {\n          const relStore = db.createObjectStore('relationships', { keyPath: 'id' });\n          relStore.createIndex('source', 'source', { unique: false });\n          relStore.createIndex('target', 'target', { unique: false });\n          relStore.createIndex('created', 'created', { unique: false });\n        }\n\n        // Object Store: articles (captured content)\n        if (!db.objectStoreNames.contains('articles')) {\n          const articleStore = db.createObjectStore('articles', { keyPath: 'id' });\n          articleStore.createIndex('url', 'url', { unique: false });\n          articleStore.createIndex('capturedAt', 'capturedAt', { unique: false });\n        }\n\n        // Object Store: settings (user preferences)\n        if (!db.objectStoreNames.contains('settings')) {\n          db.createObjectStore('settings', { keyPath: 'id' });\n        }\n\n        // Object Store: syntheses (weekly learning summaries)\n        if (!db.objectStoreNames.contains('syntheses')) {\n          const synthesisStore = db.createObjectStore('syntheses', { keyPath: 'id' });\n          synthesisStore.createIndex('createdAt', 'createdAt', { unique: false });\n          synthesisStore.createIndex('weekStart', 'weekStart', { unique: false });\n        }\n      };\n    });\n  }\n\n  async get(storeName, id) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readonly');\n      const request = tx.objectStore(storeName).get(id);\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  async add(storeName, item) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readwrite');\n      const request = tx.objectStore(storeName).add(item);\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  async update(storeName, item) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readwrite');\n      const request = tx.objectStore(storeName).put(item);\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  async delete(storeName, id) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readwrite');\n      const request = tx.objectStore(storeName).delete(id);\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  async getAll(storeName) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readonly');\n      const request = tx.objectStore(storeName).getAll();\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  async queryByIndex(storeName, indexName, value) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readonly');\n      const index = tx.objectStore(storeName).index(indexName);\n      const request = index.getAll(value);\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  async clear(storeName) {\n    await this.init();\n    return new Promise((resolve, reject) => {\n      const tx = this.db.transaction([storeName], 'readwrite');\n      const request = tx.objectStore(storeName).clear();\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n}\n\n// Singleton instance\nlet dbInstance = null;\n\nexport function getDB() {\n  if (!dbInstance) {\n    dbInstance = new Database();\n  }\n  return dbInstance;\n}\n\nexport default Database;\n","// Graph-specific storage operations (optimized with relationship index)\nimport { getDB } from './db.js';\n\nclass GraphStore {\n  constructor() {\n    this.db = getDB();\n    this.relationshipIndex = new Map();  // \"source-target\" → relationship (O(1) lookups)\n  }\n\n  async init() {\n    await this.db.init();\n    await this.buildRelationshipIndex();\n  }\n\n  /**\n   * Build in-memory index of relationships for O(1) duplicate detection\n   * Called on initialization and after bulk operations\n   */\n  async buildRelationshipIndex() {\n    const all = await this.db.getAll('relationships');\n    this.relationshipIndex.clear();\n\n    for (const rel of all) {\n      const key1 = `${rel.source}-${rel.target}`;\n      const key2 = `${rel.target}-${rel.source}`;\n      this.relationshipIndex.set(key1, rel);\n      this.relationshipIndex.set(key2, rel);\n    }\n  }\n\n  // Entity operations\n  async addEntity(entity) {\n    // Check if entity with same name already exists\n    const existing = await this.db.queryByIndex('entities', 'name', entity.name);\n\n    if (existing.length > 0) {\n      // Update existing entity\n      const existingEntity = existing[0];\n      existingEntity.lastSeen = Date.now();\n      existingEntity.sources = [...new Set([...existingEntity.sources, ...entity.sources])];\n      existingEntity.metadata.frequency += 1;\n\n      await this.db.update('entities', existingEntity);\n      return existingEntity.id;\n    } else {\n      // Add new entity\n      return await this.db.add('entities', entity);\n    }\n  }\n\n  async getEntity(id) {\n    return await this.db.get('entities', id);\n  }\n\n  async getAllEntities() {\n    return await this.db.getAll('entities');\n  }\n\n  async getEntitiesByTopic(topic) {\n    return await this.db.queryByIndex('entities', 'topic', topic);\n  }\n\n  async getEntitiesByType(type) {\n    return await this.db.queryByIndex('entities', 'type', type);\n  }\n\n  // Relationship operations (optimized with O(1) index lookup)\n  async addRelationship(relationship) {\n    // Use index for O(1) duplicate detection instead of O(e) getAll\n    const key = `${relationship.source}-${relationship.target}`;\n    const duplicate = this.relationshipIndex.get(key);\n\n    if (duplicate) {\n      // Update existing relationship\n      if (relationship.strength > duplicate.strength) {\n        duplicate.strength = relationship.strength;\n        duplicate.description = relationship.description;\n      }\n      duplicate.metadata.coOccurrences += 1;\n      duplicate.metadata.contexts.push(...relationship.metadata.contexts);\n\n      await this.db.update('relationships', duplicate);\n\n      // Update index with modified relationship\n      const key1 = `${duplicate.source}-${duplicate.target}`;\n      const key2 = `${duplicate.target}-${duplicate.source}`;\n      this.relationshipIndex.set(key1, duplicate);\n      this.relationshipIndex.set(key2, duplicate);\n\n      return duplicate.id;\n    } else {\n      // Add new relationship\n      const id = await this.db.add('relationships', relationship);\n      relationship.id = id;\n\n      // Update index with new relationship\n      const key1 = `${relationship.source}-${relationship.target}`;\n      const key2 = `${relationship.target}-${relationship.source}`;\n      this.relationshipIndex.set(key1, relationship);\n      this.relationshipIndex.set(key2, relationship);\n\n      return id;\n    }\n  }\n\n  async getRelationship(id) {\n    return await this.db.get('relationships', id);\n  }\n\n  async getAllRelationships() {\n    return await this.db.getAll('relationships');\n  }\n\n  async getRelationshipsForEntity(entityId) {\n    const all = await this.db.getAll('relationships');\n    return all.filter(r => r.source === entityId || r.target === entityId);\n  }\n\n  // Article operations\n  async addArticle(article) {\n    const existing = await this.db.queryByIndex('articles', 'url', article.url);\n\n    if (existing.length > 0) {\n      console.warn(`Article already exists: ${article.url}. Updating instead.`);\n      article.id = existing[0].id;\n      await this.db.update('articles', article);\n      return existing[0].id;\n    }\n\n    return await this.db.add('articles', article);\n  }\n\n  async getArticle(id) {\n    return await this.db.get('articles', id);\n  }\n\n  async getAllArticles() {\n    return await this.db.getAll('articles');\n  }\n\n  async updateArticle(id, updates) {\n    const existing = await this.db.get('articles', id);\n    if (!existing) {\n      throw new Error(`Article with id ${id} not found`);\n    }\n\n    const updated = { ...existing, ...updates };\n    await this.db.update('articles', updated);\n    return updated;\n  }\n\n  async getRecentArticles(limit = 10) {\n    const all = await this.db.getAll('articles');\n    return all\n      .sort((a, b) => b.capturedAt - a.capturedAt)\n      .slice(0, limit);\n  }\n\n  // Settings operations\n  async getSettings() {\n    const settings = await this.db.get('settings', 'user_settings');\n\n    if (!settings) {\n      // Return defaults\n      return {\n        id: 'user_settings',\n        topics: ['AI', 'Technology', 'Science', 'Business', 'Innovation'],\n        autoCapture: false,\n        graphLayout: 'force',\n        insightFrequency: 'weekly',\n        theme: 'light',\n        privacy: {\n          storeFullText: false,\n          syncEnabled: false\n        }\n      };\n    }\n\n    return settings;\n  }\n\n  async updateSettings(settings) {\n    settings.id = 'user_settings';\n    return await this.db.update('settings', settings);\n  }\n\n  // Utility operations\n  async clearAllData() {\n    await this.db.clear('entities');\n    await this.db.clear('relationships');\n    await this.db.clear('articles');\n  }\n\n  async getStatistics() {\n    const entities = await this.db.getAll('entities');\n    const relationships = await this.db.getAll('relationships');\n    const articles = await this.db.getAll('articles');\n\n    return {\n      totalEntities: entities.length,\n      totalRelationships: relationships.length,\n      totalArticles: articles.length\n    };\n  }\n}\n\n// Singleton instance\nlet graphStoreInstance = null;\n\nexport async function getGraphStore() {\n  if (!graphStoreInstance) {\n    graphStoreInstance = new GraphStore();\n    await graphStoreInstance.init();\n  }\n  return graphStoreInstance;\n}\n\nexport default GraphStore;\n","// Core knowledge graph operations engine (optimized with adjacency list)\n// This was initially O(N^4) but after a weekend of frustration, rewrote to O(N^2)\nimport { getGraphStore } from '../storage/graph-store.js';\n\nclass GraphEngine {\n  constructor(store) {\n    this.store = store;\n    // Caching everything because the original naive approach was painfully slow\n    this.cache = {\n      entities: new Map(),\n      relationships: new Map(),\n      adjacencyList: new Map(),  // entityId → Set of neighbor IDs (O(1) lookups)\n      degreeCache: new Map(),     // entityId → degree count (O(1) lookups)\n      edgeLookup: new Map()       // 'source-target' → relationship (O(1) edge lookups)\n    };\n  }\n\n  /**\n   * Load entire graph from IndexedDB into memory cache\n   * Builds adjacency list and degree cache for O(1) graph operations\n   * @returns {Promise<Object>} Load statistics\n   */\n  async loadGraph() {\n    const entities = await this.store.getAllEntities();\n    const relationships = await this.store.getAllRelationships();\n\n    this.cache.entities.clear();\n    this.cache.relationships.clear();\n    this.cache.adjacencyList.clear();\n    this.cache.degreeCache.clear();\n    this.cache.edgeLookup.clear();\n\n    // Build entities map - O(n)\n    entities.forEach(e => this.cache.entities.set(e.id, e));\n\n    // Build relationships map and edge lookup - O(e)\n    relationships.forEach(r => {\n      this.cache.relationships.set(r.id, r);\n      // Build bidirectional edge lookup for O(1) relationship finding\n      this.cache.edgeLookup.set(`${r.source}-${r.target}`, r);\n      this.cache.edgeLookup.set(`${r.target}-${r.source}`, r);\n    });\n\n    // Build adjacency list for O(1) neighbor lookups - O(e)\n    for (const rel of relationships) {\n      if (!this.cache.adjacencyList.has(rel.source)) {\n        this.cache.adjacencyList.set(rel.source, new Set());\n      }\n      if (!this.cache.adjacencyList.has(rel.target)) {\n        this.cache.adjacencyList.set(rel.target, new Set());\n      }\n\n      this.cache.adjacencyList.get(rel.source).add(rel.target);\n      this.cache.adjacencyList.get(rel.target).add(rel.source);\n    }\n\n    // Build degree cache for O(1) degree queries - O(n)\n    for (const [entityId, neighbors] of this.cache.adjacencyList) {\n      this.cache.degreeCache.set(entityId, neighbors.size);\n    }\n\n    // Set degree to 0 for entities with no relationships\n    for (const entityId of this.cache.entities.keys()) {\n      if (!this.cache.degreeCache.has(entityId)) {\n        this.cache.degreeCache.set(entityId, 0);\n      }\n    }\n\n    return {\n      nodeCount: this.cache.entities.size,\n      edgeCount: this.cache.relationships.size\n    };\n  }\n\n  /**\n   * Invalidate caches and reload graph (call after adding relationships)\n   * @returns {Promise<Object>} Load statistics\n   */\n  async reloadGraph() {\n    return await this.loadGraph();\n  }\n\n  /**\n   * Get graph data formatted for visualization\n   * @returns {Object} Graph with nodes and links\n   */\n  getGraphData() {\n    const nodes = Array.from(this.cache.entities.values()).map(entity => ({\n      id: entity.id,\n      name: entity.name,\n      type: entity.type,\n      topic: entity.topic,\n      relevance: entity.relevance,\n      degree: this.getDegree(entity.id),\n      sources: entity.sources || []\n    }));\n\n    const links = Array.from(this.cache.relationships.values()).map(rel => ({\n      id: rel.id,\n      source: rel.source,\n      target: rel.target,\n      type: rel.type,\n      strength: rel.strength,\n      description: rel.description\n    }));\n\n    return { nodes, links };\n  }\n\n  /**\n   * Get the degree (number of connections) for an entity\n   * Optimized: O(1) lookup instead of O(e) iteration\n   * @param {string} entityId - Entity ID\n   * @returns {number} Number of connections\n   */\n  getDegree(entityId) {\n    return this.cache.degreeCache.get(entityId) || 0;\n  }\n\n  /**\n   * Get all neighboring entities for a given entity\n   * Optimized: O(1) lookup instead of O(e) iteration\n   * @param {string} entityId - Entity ID\n   * @returns {Array<string>} Array of neighbor entity IDs\n   */\n  getNeighbors(entityId) {\n    const neighbors = this.cache.adjacencyList.get(entityId);\n    return neighbors ? Array.from(neighbors) : [];\n  }\n\n  /**\n   * Get most connected entities (hubs)\n   * Optimized: O(n log n) instead of O(n * e)\n   * @param {number} limit - Number of hubs to return\n   * @returns {Array<Object>} Hub entities with degree\n   */\n  getHubs(limit = 10) {\n    const entities = Array.from(this.cache.entities.values());\n\n    // O(n) instead of O(n * e) thanks to degree cache\n    const entitiesWithDegree = entities.map(e => ({\n      ...e,\n      degree: this.getDegree(e.id)\n    }));\n\n    return entitiesWithDegree\n      .sort((a, b) => b.degree - a.degree)  // O(n log n)\n      .slice(0, limit);\n  }\n\n  /**\n   * Get all entities for a specific topic\n   * @param {string} topic - Topic name\n   * @returns {Array<Object>} Entities in that topic\n   */\n  getEntitiesByTopic(topic) {\n    const entities = Array.from(this.cache.entities.values());\n    return entities.filter(e => e.topic === topic);\n  }\n\n  /**\n   * Get subgraph around an entity (ego network)\n   * Optimized: O(1) edge lookups using edgeLookup map instead of O(e) iteration\n   * @param {string} entityId - Center entity ID\n   * @param {number} depth - How many hops to include\n   * @returns {Object} Subgraph with nodes and links\n   */\n  getSubgraph(entityId, depth = 1) {\n    const nodes = new Set([entityId]);\n    const links = [];\n    const linkIds = new Set();\n\n    let currentLevel = [entityId];\n\n    for (let d = 0; d < depth; d++) {\n      const nextLevel = [];\n\n      for (const current of currentLevel) {\n        const neighbors = this.getNeighbors(current);\n\n        for (const neighbor of neighbors) {\n          nodes.add(neighbor);\n          if (d < depth - 1) {\n            nextLevel.push(neighbor);\n          }\n\n          // O(1) relationship lookup using edgeLookup map\n          const edgeKey = `${current}-${neighbor}`;\n          const rel = this.cache.edgeLookup.get(edgeKey);\n\n          if (rel && !linkIds.has(rel.id)) {\n            links.push(rel);\n            linkIds.add(rel.id);\n          }\n        }\n      }\n\n      currentLevel = nextLevel;\n    }\n\n    return {\n      nodes: Array.from(nodes).map(id => this.cache.entities.get(id)).filter(Boolean),\n      links: links\n    };\n  }\n\n  /**\n   * Calculate graph statistics\n   * @returns {Object} Statistical summary\n   */\n  getStatistics() {\n    const entities = Array.from(this.cache.entities.values());\n    const relationships = Array.from(this.cache.relationships.values());\n\n    const entityTypeCount = {};\n    entities.forEach(e => {\n      entityTypeCount[e.type] = (entityTypeCount[e.type] || 0) + 1;\n    });\n\n    const topicCount = {};\n    entities.forEach(e => {\n      topicCount[e.topic] = (topicCount[e.topic] || 0) + 1;\n    });\n\n    // Calculate average degree using cache - O(n)\n    const totalDegree = Array.from(this.cache.degreeCache.values())\n      .reduce((sum, degree) => sum + degree, 0);\n    const avgDegree = entities.length > 0 ? totalDegree / entities.length : 0;\n\n    // Graph density\n    const possibleEdges = (entities.length * (entities.length - 1)) / 2;\n    const density = possibleEdges > 0 ? relationships.length / possibleEdges : 0;\n\n    return {\n      totalEntities: entities.length,\n      totalRelationships: relationships.length,\n      entityTypeCount,\n      topicCount,\n      avgDegree: avgDegree.toFixed(2),\n      density: density.toFixed(4),\n      hubs: this.getHubs(5).map(h => ({ name: h.name, degree: h.degree }))\n    };\n  }\n\n  /**\n   * Search entities by name (fuzzy search)\n   * @param {string} query - Search query\n   * @param {number} limit - Max results\n   * @returns {Array<Object>} Matching entities\n   */\n  searchEntities(query, limit = 10) {\n    const queryLower = query.toLowerCase();\n    const entities = Array.from(this.cache.entities.values());\n\n    const matches = entities\n      .filter(e => e.name.toLowerCase().includes(queryLower))\n      .map(e => ({\n        ...e,\n        score: this.calculateSearchScore(e, queryLower),\n        degree: this.getDegree(e.id)\n      }))\n      .sort((a, b) => b.score - a.score)\n      .slice(0, limit);\n\n    return matches;\n  }\n\n  /**\n   * Calculate search relevance score\n   * @param {Object} entity - Entity to score\n   * @param {string} query - Search query\n   * @returns {number} Relevance score\n   */\n  calculateSearchScore(entity, query) {\n    let score = 0;\n\n    if (entity.name.toLowerCase() === query) {\n      score += 10;\n    } else if (entity.name.toLowerCase().startsWith(query)) {\n      score += 5;\n    } else {\n      score += 1;\n    }\n\n    score += entity.relevance;\n    score += this.getDegree(entity.id) * 0.1;\n\n    return score;\n  }\n}\n\nlet graphEngineInstance = null;\n\n/**\n * Get singleton GraphEngine instance\n * @returns {Promise<GraphEngine>} Graph engine\n */\nexport async function getGraphEngine() {\n  if (!graphEngineInstance) {\n    const store = await getGraphStore();\n    graphEngineInstance = new GraphEngine(store);\n    await graphEngineInstance.loadGraph();\n  }\n  return graphEngineInstance;\n}\n\nexport default GraphEngine;\n","/**\n * Check if Chrome's Prompt API is available and ready\n * @returns {Promise<Object>} Availability status object\n */\nexport async function checkPromptAPIAvailability() {\n  try {\n    if (typeof LanguageModel === 'undefined') {\n      return {\n        available: false,\n        status: 'not_supported',\n        reason: 'Prompt API not available in this Chrome version',\n        action: 'Update to Chrome 128+ or enable chrome://flags/#prompt-api-for-gemini-nano'\n      };\n    }\n\n    const status = await LanguageModel.availability();\n\n    return {\n      available: status === 'readily' || status === 'available',\n      status: status,\n      needsDownload: status === 'after-download' || status === 'downloadable',\n      downloading: status === 'downloading',\n      reason: status === 'readily' || status === 'available'\n        ? 'Prompt API ready'\n        : `Model status: ${status}`,\n      action: status === 'after-download' || status === 'downloadable'\n        ? 'Model needs to be downloaded. Visit chrome://on-device-internals'\n        : null\n    };\n  } catch (error) {\n    console.error('Error checking Prompt API availability:', error);\n    return {\n      available: false,\n      status: 'error',\n      reason: error.message,\n      action: 'Check browser console for details'\n    };\n  }\n}\n\n/**\n * Check if Chrome's Summarizer API is available and ready\n * @returns {Promise<Object>} Availability status object\n */\nexport async function checkSummarizerAvailability() {\n  try {\n    if (typeof Summarizer === 'undefined') {\n      return {\n        available: false,\n        status: 'not_supported',\n        reason: 'Summarizer API not available in this Chrome version',\n        action: 'Update to Chrome 128+'\n      };\n    }\n\n    const status = await Summarizer.availability();\n\n    return {\n      available: status === 'readily' || status === 'available',\n      status: status,\n      needsDownload: status === 'after-download' || status === 'downloadable',\n      downloading: status === 'downloading',\n      reason: status === 'readily' || status === 'available'\n        ? 'Summarizer API ready'\n        : `Model status: ${status}`,\n      action: status === 'after-download' || status === 'downloadable'\n        ? 'Model needs to be downloaded'\n        : null\n    };\n  } catch (error) {\n    console.error('Error checking Summarizer API availability:', error);\n    return {\n      available: false,\n      status: 'error',\n      reason: error.message,\n      action: 'Check browser console for details'\n    };\n  }\n}\n\n/**\n * Check if Chrome's Language Detector API is available\n * @returns {Promise<Object>} Availability status object\n */\nexport async function checkLanguageDetectorAvailability() {\n  try {\n    if (typeof LanguageDetector === 'undefined') {\n      return {\n        available: false,\n        status: 'not_supported',\n        reason: 'Language Detector API not available',\n        action: 'Update to Chrome 128+'\n      };\n    }\n\n    const status = await LanguageDetector.availability();\n\n    return {\n      available: status === 'readily' || status === 'available',\n      status: status,\n      reason: status === 'readily' || status === 'available'\n        ? 'Language Detector ready'\n        : `Status: ${status}`,\n      languagesSupported: []\n    };\n  } catch (error) {\n    console.error('Error checking Language Detector availability:', error);\n    return {\n      available: false,\n      status: 'error',\n      reason: error.message,\n      action: 'Language detection may not work'\n    };\n  }\n}\n\n/**\n * Check availability of all AI APIs\n * @returns {Promise<Object>} Combined availability status\n */\nexport async function checkAllAIAvailability() {\n  const [prompt, summarizer, languageDetector] = await Promise.all([\n    checkPromptAPIAvailability(),\n    checkSummarizerAvailability(),\n    checkLanguageDetectorAvailability()\n  ]);\n\n  const allReady = prompt.available && summarizer.available && languageDetector.available;\n\n  return {\n    allReady,\n    prompt,\n    summarizer,\n    languageDetector,\n    message: allReady\n      ? 'All AI APIs are ready'\n      : 'Some AI APIs are not available. Extension will use fallback methods.'\n  };\n}\n\n/**\n * Wait for AI model to become available (with timeout)\n * @param {Function} checkFunction - Availability check function\n * @param {number} timeoutMs - Maximum wait time in milliseconds\n * @param {number} pollIntervalMs - Check interval in milliseconds\n * @returns {Promise<boolean>} True if became available, false if timeout\n */\nexport async function waitForAIAvailability(\n  checkFunction,\n  timeoutMs = 30000,\n  pollIntervalMs = 1000\n) {\n  const startTime = Date.now();\n\n  while (Date.now() - startTime < timeoutMs) {\n    const status = await checkFunction();\n\n    if (status.available) {\n      return true;\n    }\n\n    if (status.status === 'not_supported' || status.status === 'error') {\n      return false;\n    }\n\n    await new Promise(resolve => setTimeout(resolve, pollIntervalMs));\n  }\n\n  return false;\n}\n\nexport default {\n  checkPromptAPIAvailability,\n  checkSummarizerAvailability,\n  checkLanguageDetectorAvailability,\n  checkAllAIAvailability,\n  waitForAIAvailability\n};\n","// Entity types for knowledge graph nodes\nexport const ENTITY_TYPES = {\n  PERSON: 'person',\n  COMPANY: 'company',\n  TECHNOLOGY: 'technology',\n  CONCEPT: 'concept'\n};\n\n// Relationship types for graph connections\nexport const RELATIONSHIP_TYPES = {\n  DIRECT: 'direct',\n  CONCEPTUAL: 'conceptual',\n  TEMPORAL: 'temporal',\n  CAUSAL: 'causal'\n};\n\n// Chrome AI API configuration\nexport const AI_CONFIG = {\n  MAX_ARTICLE_CHARS: 6000,\n  MAX_ENTITIES_PER_ARTICLE: 8,\n  MAX_RELATIONSHIP_CANDIDATES: 8,\n  ENTITY_EXTRACTION_TEMPERATURE: 0.3,\n  RELATIONSHIP_INFERENCE_TEMPERATURE: 0.3,\n  SUMMARIZER_LENGTH: 'short',\n  SUMMARIZER_TYPE: 'key-points',\n  SUMMARIZER_FORMAT: 'markdown'\n};\n\n// Graph visualization settings\nexport const GRAPH_CONFIG = {\n  CANVAS_THRESHOLD: 200,\n  FORCE_STRENGTH: -30,\n  FORCE_DISTANCE: 100,\n  COLLISION_RADIUS: 20,\n  DEFAULT_NODE_SIZE: 8,\n  SELECTED_NODE_SIZE: 12,\n  LINK_WIDTH: 1.5\n};\n\n// UI layout configuration\nexport const UI_CONFIG = {\n  POPUP_WIDTH: 800,\n  POPUP_HEIGHT: 600,\n  INSIGHTS_LIMIT: 5,\n  RECENT_ARTICLES_LIMIT: 10\n};\n\n// Default user interests\nexport const DEFAULT_TOPICS = [\n  'AI',\n  'Technology',\n  'Science',\n  'Business',\n  'Innovation'\n];\n\n// Storage configuration\nexport const STORAGE_KEYS = {\n  USER_SETTINGS: 'user_settings'\n};\n\nexport default {\n  ENTITY_TYPES,\n  RELATIONSHIP_TYPES,\n  AI_CONFIG,\n  GRAPH_CONFIG,\n  UI_CONFIG,\n  DEFAULT_TOPICS,\n  STORAGE_KEYS\n};\n","// Chrome AI API wrappers for Prompt API and Summarizer API\n// Had to add queuing because Chrome AI gets cranky with too many concurrent requests\nimport { checkPromptAPIAvailability, checkSummarizerAvailability } from './availability.js';\nimport { AI_CONFIG } from './constants.js';\n\n// AI Request Queue to prevent overwhelming Chrome AI with concurrent requests\n// Learned this the hard way - 6 simultaneous requests = timeouts and sadness\nclass AIRequestQueue {\n  constructor(maxConcurrent = 2) {\n    this.maxConcurrent = maxConcurrent; // Sweet spot seems to be 2\n    this.running = 0;\n    this.queue = [];\n  }\n\n  async add(requestFunction) {\n    return new Promise((resolve, reject) => {\n      this.queue.push({ requestFunction, resolve, reject });\n      this.processQueue();\n    });\n  }\n\n  async processQueue() {\n    if (this.running >= this.maxConcurrent || this.queue.length === 0) {\n      return;\n    }\n\n    this.running++;\n    const { requestFunction, resolve, reject } = this.queue.shift();\n\n    console.log(`[AI QUEUE] Starting request (${this.running}/${this.maxConcurrent} running, ${this.queue.length} queued)`);\n\n    try {\n      const result = await requestFunction();\n      resolve(result);\n    } catch (error) {\n      reject(error);\n    } finally {\n      this.running--;\n      console.log(`[AI QUEUE] Request completed (${this.running}/${this.maxConcurrent} running, ${this.queue.length} queued)`);\n      // Process next item in queue\n      setTimeout(() => this.processQueue(), 0);\n    }\n  }\n}\n\n// Global AI request queue instance\nconst aiQueue = new AIRequestQueue(2); // Limit to 2 concurrent AI requests\n\n/**\n * Create a Prompt API session\n * @param {string} systemPrompt - System instructions for the AI\n * @param {Object} options - Session options\n * @returns {Promise<Object>} AI session object\n */\nexport async function createPromptSession(systemPrompt, options = {}) {\n  const availability = await checkPromptAPIAvailability();\n\n  if (!availability.available) {\n    throw new Error(`Prompt API not available: ${availability.reason}`);\n  }\n\n  try {\n    const session = await LanguageModel.create({\n      systemPrompt: systemPrompt,\n      temperature: options.temperature || 0.7,\n      topK: options.topK || 40\n    });\n\n    return session;\n  } catch (error) {\n    console.error('Failed to create Prompt API session:', error);\n    throw new Error(`Session creation failed: ${error.message}`);\n  }\n}\n\n/**\n * Send a single prompt and get response (auto-cleanup session)\n * @param {string} systemPrompt - System instructions\n * @param {string} userPrompt - User query\n * @param {Object} options - Session options\n * @returns {Promise<string>} AI response\n */\nexport async function promptOnce(systemPrompt, userPrompt, options = {}) {\n  // Queue the AI request to prevent overwhelming Chrome AI\n  return aiQueue.add(async () => {\n    let session = null;\n\n    try {\n      session = await createPromptSession(systemPrompt, options);\n\n      // Increased timeout from 20s to 30s for better success rate\n      const timeoutPromise = new Promise((_, reject) =>\n        setTimeout(() => reject(new Error('AI request timed out after 30 seconds')), 30000)\n      );\n\n      const response = await Promise.race([\n        session.prompt(userPrompt),\n        timeoutPromise\n      ]);\n\n      return response;\n    } catch (error) {\n      console.error('Prompt failed:', error);\n      throw error;\n    } finally {\n      if (session) {\n        session.destroy();\n      }\n    }\n  });\n}\n\n/**\n * Send a prompt and get streaming response\n * @param {string} systemPrompt - System instructions\n * @param {string} userPrompt - User query\n * @param {Object} options - Session options\n * @returns {AsyncGenerator<string>} Streaming response chunks\n */\nexport async function* promptStreaming(systemPrompt, userPrompt, options = {}) {\n  let session = null;\n\n  try {\n    session = await createPromptSession(systemPrompt, options);\n    const stream = await session.promptStreaming(userPrompt);\n\n    for await (const chunk of stream) {\n      yield chunk;\n    }\n  } catch (error) {\n    console.error('Streaming prompt failed:', error);\n    throw error;\n  } finally {\n    if (session) {\n      session.destroy();\n    }\n  }\n}\n\n/**\n * Summarize text using Chrome's Summarizer API\n * @param {string} text - Text to summarize\n * @param {Object} options - Summarizer options\n * @returns {Promise<string>} Summary text\n */\nexport async function summarizeText(text, options = {}) {\n  const availability = await checkSummarizerAvailability();\n\n  if (!availability.available) {\n    console.warn('Summarizer not available, returning truncated text');\n    return text.slice(0, 1000) + '...';\n  }\n\n  // Queue the summarizer request to prevent overwhelming Chrome AI\n  return aiQueue.add(async () => {\n    let summarizer = null;\n\n    try {\n      summarizer = await Summarizer.create({\n        type: options.type || AI_CONFIG.SUMMARIZER_TYPE,\n        format: options.format || AI_CONFIG.SUMMARIZER_FORMAT,\n        length: options.length || AI_CONFIG.SUMMARIZER_LENGTH\n      });\n\n      const summary = await summarizer.summarize(text);\n      return summary;\n    } catch (error) {\n      console.error('Summarization failed:', error);\n      return text.slice(0, 1000) + '...';\n    } finally {\n      if (summarizer) {\n        summarizer.destroy();\n      }\n    }\n  });\n}\n\n/**\n * Detect language of text using Chrome's Language Detector API\n * @param {string} text - Text to analyze\n * @returns {Promise<Object>} Language detection result\n */\nexport async function detectLanguage(text) {\n  try {\n    if (typeof LanguageDetector === 'undefined') {\n      return { language: 'en', confidence: 0.5 };\n    }\n\n    const detector = await LanguageDetector.create();\n    const results = await detector.detect(text);\n    detector.destroy();\n\n    if (results && results.length > 0) {\n      return {\n        language: results[0].detectedLanguage,\n        confidence: results[0].confidence\n      };\n    }\n\n    return { language: 'en', confidence: 0.5 };\n  } catch (error) {\n    console.error('Language detection failed:', error);\n    return { language: 'en', confidence: 0.5 };\n  }\n}\n\n/**\n * Parse JSON from AI response (handles markdown wrapping)\n * @param {string} response - AI response text\n * @returns {Object} Parsed JSON object\n */\nexport function parseAIJSON(response) {\n  try {\n    const cleaned = response\n      .replace(/```json\\n?/g, '')\n      .replace(/```\\n?/g, '')\n      .trim();\n\n    return JSON.parse(cleaned);\n  } catch (error) {\n    console.error('Failed to parse AI JSON response:', error);\n    // console.log('Raw response:', response);\n    throw new Error('Invalid JSON in AI response');\n  }\n}\n\n/**\n * Custom error class for AI-related errors\n */\nexport class AIError extends Error {\n  constructor(message, type, originalError) {\n    super(message);\n    this.name = 'AIError';\n    this.type = type;\n    this.originalError = originalError;\n  }\n}\n\n/**\n * Wrap AI API calls with error handling and fallback\n * @param {Function} apiCall - Function that makes the AI API call\n * @param {Function} fallback - Fallback function if API fails\n * @returns {Promise<any>} Result from API or fallback\n */\nexport async function withAIErrorHandling(apiCall, fallback) {\n  try {\n    return await apiCall();\n  } catch (error) {\n    console.error('AI API call failed:', error);\n\n    if (error.message.includes('not available')) {\n      throw new AIError(\n        'AI model not available. Please download it from chrome://on-device-internals',\n        'unavailable',\n        error\n      );\n    } else if (error.message.includes('rate limit')) {\n      throw new AIError(\n        'Too many requests. Please wait a moment.',\n        'rate_limit',\n        error\n      );\n    } else if (error.message.includes('timeout')) {\n      throw new AIError(\n        'Request timed out. Try again.',\n        'timeout',\n        error\n      );\n    } else {\n      if (fallback) {\n        console.warn('Using fallback method due to AI error');\n        return await fallback();\n      }\n      throw error;\n    }\n  }\n}\n\nexport default {\n  createPromptSession,\n  promptOnce,\n  promptStreaming,\n  summarizeText,\n  detectLanguage,\n  parseAIJSON,\n  AIError,\n  withAIErrorHandling\n};\n","// Text processing utilities for article content\n\nexport function cleanText(text) {\n  if (!text) return '';\n\n  return text\n    .replace(/\\s+/g, ' ')\n    .replace(/\\n{3,}/g, '\\n\\n')\n    .trim();\n}\n\nexport function truncateText(text, maxLength) {\n  if (!text) return '';\n  if (text.length <= maxLength) return text;\n\n  return `${text.slice(0, maxLength)}...`;\n}\n\nexport function extractArticleText() {\n  // Try to find article content using common selectors\n  const selectors = [\n    'article',\n    '[role=\"main\"]',\n    '.article-content',\n    '.post-content',\n    '.entry-content',\n    'main'\n  ];\n\n  for (const selector of selectors) {\n    const element = document.querySelector(selector);\n    if (element && element.textContent.length > 500) {\n      return cleanText(element.textContent);\n    }\n  }\n\n  // Fallback to body text\n  return cleanText(document.body.textContent);\n}\n\nexport function countWords(text) {\n  if (!text) return 0;\n  return text.split(/\\s+/).filter(word => word.length > 0).length;\n}\n\nexport function extractDomain(url) {\n  try {\n    return new URL(url).hostname;\n  } catch (error) {\n    return '';\n  }\n}\n\nexport function sanitizeForJSON(text) {\n  if (!text) return '';\n\n  return text\n    .replace(/[\\n\\r]/g, ' ')\n    .replace(/\"/g, '\\\\\"')\n    .trim();\n}\n\nexport function extractKeywords(text, limit = 10) {\n  // Simple keyword extraction based on frequency\n  const words = text\n    .toLowerCase()\n    .split(/\\W+/)\n    .filter(word => word.length > 3);\n\n  const wordCounts = {};\n  words.forEach(word => {\n    wordCounts[word] = (wordCounts[word] || 0) + 1;\n  });\n\n  return Object.entries(wordCounts)\n    .sort((a, b) => b[1] - a[1])\n    .slice(0, limit)\n    .map(([word]) => word);\n}\n\nexport default {\n  cleanText,\n  truncateText,\n  extractArticleText,\n  countWords,\n  extractDomain,\n  sanitizeForJSON,\n  extractKeywords\n};\n","// Entity extraction from article text using Chrome's Prompt API\nimport { promptOnce, summarizeText, parseAIJSON, withAIErrorHandling } from '../utils/ai-helpers.js';\nimport { AI_CONFIG, ENTITY_TYPES } from '../utils/constants.js';\nimport { truncateText, extractKeywords } from '../utils/text-processing.js';\n\n// System prompt for entity extraction\nconst ENTITY_EXTRACTION_SYSTEM_PROMPT = `You are an expert at extracting key entities from articles.\nExtract the most important entities mentioned in the text.\n\nEntity types:\n- person: Individual people (e.g., \"Elon Musk\", \"Marie Curie\")\n- company: Organizations and companies (e.g., \"Google\", \"NASA\")\n- technology: Technologies, tools, frameworks (e.g., \"Gemini\", \"React\", \"Quantum Computing\")\n- concept: Ideas, theories, methodologies (e.g., \"Machine Learning\", \"Climate Change\")\n\nIMPORTANT - Do NOT extract:\n- Pronouns, demonstratives, or grammar words (this, that, these, those, it, he, she, they)\n- Generic nouns (thing, stuff, way, type, kind, people, person)\n- Time references (year, day, week, month, time)\n- Vague descriptors (new, old, good, bad, great, small, large)\n- Common verbs or adjectives used alone\n- Single-letter entities or numbers\n- Generic processes (system, method, process, approach)\n\nOnly extract SPECIFIC, NAMED entities that are meaningful and relevant.\n\nReturn ONLY valid JSON, no markdown formatting, no explanation.\nFocus on entities relevant to the user's interests.`;\n\n/**\n * Extract entities from article text using AI\n * @param {string} articleText - Full article text\n * @param {Array<string>} userTopics - User's topics of interest\n * @returns {Promise<Array<Object>>} Extracted entities\n */\nexport async function extractEntities(articleText, userTopics = []) {\n  const truncatedText = truncateText(articleText, AI_CONFIG.MAX_ARTICLE_CHARS);\n\n  const userPrompt = buildExtractionPrompt(truncatedText, userTopics);\n\n  try {\n    return await withAIErrorHandling(\n      async () => {\n        const response = await promptOnce(\n          ENTITY_EXTRACTION_SYSTEM_PROMPT,\n          userPrompt,\n          { temperature: AI_CONFIG.ENTITY_EXTRACTION_TEMPERATURE }\n        );\n\n        const entities = parseAIJSON(response);\n\n        return validateAndFilterEntities(entities, userTopics);\n      },\n      async () => {\n        console.warn('AI extraction failed, using fallback keyword extraction');\n        return fallbackKeywordExtraction(articleText, userTopics);\n      }\n    );\n  } catch (error) {\n    console.error('Entity extraction completely failed:', error);\n    return fallbackKeywordExtraction(articleText, userTopics);\n  }\n}\n\n/**\n * Extract entities from article with summarization for long texts\n * @param {string} articleText - Full article text\n * @param {Array<string>} userTopics - User's topics of interest\n * @returns {Promise<Array<Object>>} Extracted entities\n */\nexport async function extractEntitiesWithSummarization(articleText, userTopics = []) {\n  let textToProcess = articleText;\n\n  if (articleText.length > AI_CONFIG.MAX_ARTICLE_CHARS) {\n    // console.log('Article is long, summarizing first...');\n    try {\n      const summary = await summarizeText(articleText, {\n        type: 'key-points',\n        length: 'medium'\n      });\n      textToProcess = summary;\n    } catch (error) {\n      console.error('Summarization failed, using truncated text:', error);\n      textToProcess = truncateText(articleText, AI_CONFIG.MAX_ARTICLE_CHARS);\n    }\n  }\n\n  return extractEntities(textToProcess, userTopics);\n}\n\n/**\n * Build the user prompt for entity extraction\n * @param {string} text - Article text\n * @param {Array<string>} topics - User topics\n * @returns {string} Formatted prompt\n */\nfunction buildExtractionPrompt(text, topics) {\n  const topicsText = topics.length > 0\n    ? topics.join(', ')\n    : 'general knowledge';\n\n  return `Article text:\n${text}\n\nUser interests: ${topicsText}\n\nExtract up to ${AI_CONFIG.MAX_ENTITIES_PER_ARTICLE} most relevant entities.\n\nReturn JSON array in this exact format:\n[\n  {\n    \"name\": \"Entity Name\",\n    \"type\": \"person|company|technology|concept\",\n    \"relevance\": 0.8,\n    \"context\": \"Brief context from article (one sentence)\"\n  }\n]\n\nFocus on entities related to: ${topicsText}`;\n}\n\n/**\n * Validate and filter extracted entities\n * @param {Array<Object>} entities - Raw entities from AI\n * @param {Array<string>} userTopics - User topics for relevance\n * @returns {Array<Object>} Validated entities\n */\nfunction validateAndFilterEntities(entities, userTopics) {\n  if (!Array.isArray(entities)) {\n    throw new Error('Entity extraction did not return an array');\n  }\n\n  const validEntityTypes = Object.values(ENTITY_TYPES);\n\n  const stopwords = new Set([\n    // Pronouns\n    'i', 'me', 'my', 'mine', 'myself',\n    'you', 'your', 'yours', 'yourself', 'yourselves',\n    'he', 'him', 'his', 'himself',\n    'she', 'her', 'hers', 'herself',\n    'it', 'its', 'itself',\n    'we', 'us', 'our', 'ours', 'ourselves',\n    'they', 'them', 'their', 'theirs', 'themselves',\n    // Demonstratives\n    'this', 'that', 'these', 'those',\n    // Interrogatives\n    'which', 'what', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n    // Quantifiers\n    'some', 'any', 'many', 'few', 'several', 'most', 'all', 'each', 'every',\n    'much', 'more', 'less', 'little', 'lot', 'none', 'both', 'either', 'neither',\n    // Common generics\n    'other', 'another', 'such', 'same', 'different', 'various', 'certain',\n    'example', 'examples', 'thing', 'things', 'stuff', 'item', 'items',\n    'way', 'ways', 'type', 'types', 'kind', 'kinds', 'sort', 'sorts',\n    'people', 'person', 'someone', 'anyone', 'everyone', 'no one', 'nobody',\n    'something', 'anything', 'everything', 'nothing',\n    'time', 'times', 'year', 'years', 'day', 'days', 'week', 'weeks', 'month', 'months',\n    'part', 'parts', 'piece', 'pieces', 'portion', 'section', 'segment',\n    'place', 'places', 'area', 'areas', 'location', 'locations',\n    'number', 'numbers', 'amount', 'amounts', 'quantity', 'quantities',\n    // Articles and determiners\n    'the', 'a', 'an', 'some', 'any',\n    // Common verbs (often extracted by mistake)\n    'is', 'are', 'was', 'were', 'be', 'been', 'being',\n    'have', 'has', 'had', 'having',\n    'do', 'does', 'did', 'doing', 'done',\n    'can', 'could', 'will', 'would', 'should', 'may', 'might', 'must',\n    'get', 'gets', 'got', 'getting', 'make', 'makes', 'made', 'making',\n    'go', 'goes', 'went', 'going', 'come', 'comes', 'came', 'coming',\n    'take', 'takes', 'took', 'taking', 'use', 'uses', 'used', 'using',\n    'find', 'finds', 'found', 'finding', 'give', 'gives', 'gave', 'giving',\n    'tell', 'tells', 'told', 'telling', 'work', 'works', 'worked', 'working',\n    'call', 'calls', 'called', 'calling', 'try', 'tries', 'tried', 'trying',\n    'ask', 'asks', 'asked', 'asking', 'need', 'needs', 'needed', 'needing',\n    'feel', 'feels', 'felt', 'feeling', 'become', 'becomes', 'became', 'becoming',\n    'leave', 'leaves', 'left', 'leaving', 'put', 'puts', 'putting',\n    // Common adjectives (too vague alone)\n    'new', 'old', 'good', 'bad', 'great', 'small', 'large', 'big', 'little',\n    'high', 'low', 'long', 'short', 'early', 'late', 'young', 'old',\n    'important', 'possible', 'available', 'likely', 'able', 'free',\n    'sure', 'certain', 'clear', 'whole', 'full', 'real', 'true', 'false',\n    'better', 'best', 'worse', 'worst', 'less', 'more', 'least', 'most',\n    // Generic nouns (too broad)\n    'system', 'systems', 'process', 'processes', 'method', 'methods',\n    'issue', 'issues', 'problem', 'problems', 'solution', 'solutions',\n    'information', 'data', 'result', 'results', 'fact', 'facts',\n    'idea', 'ideas', 'thought', 'thoughts', 'view', 'views',\n    'point', 'points', 'case', 'cases', 'question', 'questions',\n    'answer', 'answers', 'reason', 'reasons', 'cause', 'causes',\n    'effect', 'effects', 'change', 'changes', 'difference', 'differences',\n    'group', 'groups', 'level', 'levels', 'order', 'orders',\n    'form', 'forms', 'state', 'states', 'line', 'lines',\n    'side', 'sides', 'end', 'ends', 'beginning', 'start',\n    'matter', 'matters', 'sense', 'senses', 'purpose', 'purposes',\n    // Adverbs\n    'very', 'really', 'quite', 'too', 'so', 'just', 'only', 'even',\n    'well', 'also', 'still', 'never', 'always', 'often', 'sometimes',\n    'here', 'there', 'now', 'then', 'today', 'yesterday', 'tomorrow',\n    'up', 'down', 'out', 'in', 'off', 'on', 'over', 'under',\n    // Prepositions and conjunctions\n    'of', 'to', 'for', 'with', 'on', 'at', 'from', 'by', 'about',\n    'as', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n    'between', 'among', 'against', 'without', 'within',\n    'and', 'or', 'but', 'if', 'because', 'while', 'although', 'unless',\n    // Common filler words\n    'etc', 'et cetera', 'versus', 'via', 'per', 'ie', 'eg', 'aka',\n    // Vague business/tech terms\n    'approach', 'strategy', 'model', 'framework', 'concept', 'theory',\n    'practice', 'technique', 'tool', 'feature', 'option', 'aspect',\n    'factor', 'element', 'component', 'detail', 'details',\n    'overview', 'summary', 'introduction', 'conclusion',\n    // Single letters and digits\n    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n    '1', '2', '3', '4', '5', '6', '7', '8', '9', '0'\n  ]);\n\n  const MIN_RELEVANCE_THRESHOLD = 0.3;\n\n  return entities\n    .filter(entity => {\n      if (!entity.name || typeof entity.name !== 'string') return false;\n      if (!entity.type || !validEntityTypes.includes(entity.type)) return false;\n      if (entity.name.length < 2 || entity.name.length > 100) return false;\n\n      const nameLower = entity.name.toLowerCase().trim();\n      if (stopwords.has(nameLower)) return false;\n\n      const words = nameLower.split(/\\s+/);\n      if (words.every(word => stopwords.has(word))) return false;\n\n      if (!/[a-zA-Z]/.test(entity.name)) return false;\n\n      const relevance = entity.relevance || 0.5;\n      if (relevance < MIN_RELEVANCE_THRESHOLD) return false;\n\n      return true;\n    })\n    .map(entity => ({\n      name: entity.name.trim(),\n      type: entity.type,\n      relevance: Math.max(0, Math.min(1, entity.relevance || 0.5)),\n      context: entity.context || '',\n      topic: assignTopic(entity, userTopics)\n    }))\n    .sort((a, b) => b.relevance - a.relevance)\n    .slice(0, AI_CONFIG.MAX_ENTITIES_PER_ARTICLE);\n}\n\n/**\n * Assign a topic to an entity based on user interests\n * @param {Object} entity - Entity object\n * @param {Array<string>} userTopics - User's topics\n * @returns {string} Assigned topic\n */\nfunction assignTopic(entity, userTopics) {\n  if (userTopics.length === 0) return 'General';\n\n  const entityText = `${entity.name} ${entity.context}`.toLowerCase();\n\n  for (const topic of userTopics) {\n    if (entityText.includes(topic.toLowerCase())) {\n      return topic;\n    }\n  }\n\n  return userTopics[0];\n}\n\n/**\n * Fallback keyword extraction when AI is unavailable\n * @param {string} text - Article text\n * @param {Array<string>} userTopics - User topics\n * @returns {Array<Object>} Extracted entities\n */\nfunction fallbackKeywordExtraction(text, userTopics) {\n  const keywords = extractKeywords(text, AI_CONFIG.MAX_ENTITIES_PER_ARTICLE);\n\n  return keywords.map(keyword => ({\n    name: keyword.charAt(0).toUpperCase() + keyword.slice(1),\n    type: ENTITY_TYPES.CONCEPT,\n    relevance: 0.5,\n    context: 'Extracted by keyword frequency',\n    topic: userTopics[0] || 'General'\n  }));\n}\n\n/**\n * Batch extract entities from multiple articles\n * @param {Array<Object>} articles - Array of article objects\n * @param {Array<string>} userTopics - User topics\n * @returns {Promise<Array<Object>>} Results for each article\n */\nexport async function batchExtractEntities(articles, userTopics) {\n  const results = [];\n\n  for (const article of articles) {\n    try {\n      const entities = await extractEntitiesWithSummarization(\n        article.text,\n        userTopics\n      );\n\n      results.push({\n        articleId: article.id,\n        success: true,\n        entities: entities,\n        count: entities.length\n      });\n    } catch (error) {\n      console.error(`Failed to extract entities from article ${article.id}:`, error);\n      results.push({\n        articleId: article.id,\n        success: false,\n        error: error.message,\n        entities: []\n      });\n    }\n\n    await new Promise(resolve => setTimeout(resolve, 100));\n  }\n\n  return results;\n}\n\nexport default {\n  extractEntities,\n  extractEntitiesWithSummarization,\n  batchExtractEntities\n};\n","const randomUUID = typeof crypto !== 'undefined' && crypto.randomUUID && crypto.randomUUID.bind(crypto);\nexport default {\n  randomUUID\n};","// Unique ID creation requires a high quality random # generator. In the browser we therefore\n// require the crypto API and do not support built-in fallback to lower quality random number\n// generators (like Math.random()).\nlet getRandomValues;\nconst rnds8 = new Uint8Array(16);\nexport default function rng() {\n  // lazy load so that environments that need to polyfill have a chance to do so\n  if (!getRandomValues) {\n    // getRandomValues needs to be invoked in a context where \"this\" is a Crypto implementation.\n    getRandomValues = typeof crypto !== 'undefined' && crypto.getRandomValues && crypto.getRandomValues.bind(crypto);\n\n    if (!getRandomValues) {\n      throw new Error('crypto.getRandomValues() not supported. See https://github.com/uuidjs/uuid#getrandomvalues-not-supported');\n    }\n  }\n\n  return getRandomValues(rnds8);\n}","import validate from './validate.js';\n/**\n * Convert array of 16 byte values to UUID string format of the form:\n * XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n */\n\nconst byteToHex = [];\n\nfor (let i = 0; i < 256; ++i) {\n  byteToHex.push((i + 0x100).toString(16).slice(1));\n}\n\nexport function unsafeStringify(arr, offset = 0) {\n  // Note: Be careful editing this code!  It's been tuned for performance\n  // and works in ways you may not expect. See https://github.com/uuidjs/uuid/pull/434\n  return byteToHex[arr[offset + 0]] + byteToHex[arr[offset + 1]] + byteToHex[arr[offset + 2]] + byteToHex[arr[offset + 3]] + '-' + byteToHex[arr[offset + 4]] + byteToHex[arr[offset + 5]] + '-' + byteToHex[arr[offset + 6]] + byteToHex[arr[offset + 7]] + '-' + byteToHex[arr[offset + 8]] + byteToHex[arr[offset + 9]] + '-' + byteToHex[arr[offset + 10]] + byteToHex[arr[offset + 11]] + byteToHex[arr[offset + 12]] + byteToHex[arr[offset + 13]] + byteToHex[arr[offset + 14]] + byteToHex[arr[offset + 15]];\n}\n\nfunction stringify(arr, offset = 0) {\n  const uuid = unsafeStringify(arr, offset); // Consistency check for valid UUID.  If this throws, it's likely due to one\n  // of the following:\n  // - One or more input array values don't map to a hex octet (leading to\n  // \"undefined\" in the uuid)\n  // - Invalid input values for the RFC `version` or `variant` fields\n\n  if (!validate(uuid)) {\n    throw TypeError('Stringified UUID is invalid');\n  }\n\n  return uuid;\n}\n\nexport default stringify;","import native from './native.js';\nimport rng from './rng.js';\nimport { unsafeStringify } from './stringify.js';\n\nfunction v4(options, buf, offset) {\n  if (native.randomUUID && !buf && !options) {\n    return native.randomUUID();\n  }\n\n  options = options || {};\n  const rnds = options.random || (options.rng || rng)(); // Per 4.4, set bits for version and `clock_seq_hi_and_reserved`\n\n  rnds[6] = rnds[6] & 0x0f | 0x40;\n  rnds[8] = rnds[8] & 0x3f | 0x80; // Copy bytes to buffer, if provided\n\n  if (buf) {\n    offset = offset || 0;\n\n    for (let i = 0; i < 16; ++i) {\n      buf[offset + i] = rnds[i];\n    }\n\n    return buf;\n  }\n\n  return unsafeStringify(rnds);\n}\n\nexport default v4;","// General helper utilities\nimport { v4 as uuidv4 } from 'uuid';\n\nexport function generateUUID() {\n  return uuidv4();\n}\n\nexport function createEntity(name, type, topic, context) {\n  return {\n    id: generateUUID(),\n    name: name.trim(),\n    type: type,\n    topic: topic,\n    relevance: 0.5,\n    firstSeen: Date.now(),\n    lastSeen: Date.now(),\n    sources: [],\n    metadata: {\n      context: context || '',\n      aliases: [],\n      frequency: 1\n    }\n  };\n}\n\nexport function createRelationship(sourceId, targetId, type, description, strength) {\n  return {\n    id: generateUUID(),\n    source: sourceId,\n    target: targetId,\n    type: type,\n    strength: Math.max(0, Math.min(1, strength)),\n    description: description,\n    inferredBy: 'ai',\n    created: Date.now(),\n    sources: [],\n    metadata: {\n      coOccurrences: 1,\n      contexts: []\n    }\n  };\n}\n\nexport function createArticle(url, title, text, summary, language, entityIds) {\n  const wordCount = text.split(/\\s+/).length;\n\n  return {\n    id: generateUUID(),\n    url: url,\n    title: title,\n    summary: summary,\n    fullText: text.length > 10000 ? null : text,\n    language: language,\n    capturedAt: Date.now(),\n    wordCount: wordCount,\n    entities: entityIds,\n    metadata: {\n      domain: extractDomain(url),\n      author: null,\n      publishedDate: null\n    }\n  };\n}\n\nfunction extractDomain(url) {\n  try {\n    return new URL(url).hostname;\n  } catch (error) {\n    return '';\n  }\n}\n\nexport function debounce(func, wait) {\n  let timeout;\n  return function executedFunction(...args) {\n    const later = () => {\n      clearTimeout(timeout);\n      func(...args);\n    };\n    clearTimeout(timeout);\n    timeout = setTimeout(later, wait);\n  };\n}\n\nexport function throttle(func, limit) {\n  let inThrottle;\n  return function executedFunction(...args) {\n    if (!inThrottle) {\n      func(...args);\n      inThrottle = true;\n      setTimeout(() => inThrottle = false, limit);\n    }\n  };\n}\n\nexport default {\n  generateUUID,\n  createEntity,\n  createRelationship,\n  createArticle,\n  debounce,\n  throttle\n};\n","// Relationship discovery using PMI (Pointwise Mutual Information) + AI semantic labeling\nimport { getGraphStore } from '../storage/graph-store.js';\nimport { createRelationship } from '../utils/helpers.js';\n\n/**\n * Find relationships between entities using PMI algorithm\n * PMI measures how much the co-occurrence of two entities exceeds random chance\n * @param {Array<Object>} entities - Entities to analyze\n * @param {Object} options - Configuration options\n * @returns {Promise<Array<Object>>} Discovered relationships\n */\nexport async function findRelationships(entities, options = {}) {\n  const {\n    minStrength = 0.3,      // Minimum PMI strength to create relationship\n    useSemanticLabeling = true,  // Use AI to label relationship types\n    maxRelationships = 50   // Max relationships to return\n  } = options;\n\n  if (entities.length < 2) {\n    return [];\n  }\n\n  try {\n    const store = await getGraphStore();\n\n    // Get all articles to analyze co-occurrence patterns\n    const allArticles = await store.getAllArticles();\n\n    if (allArticles.length === 0) {\n      return [];\n    }\n\n    // Build co-occurrence matrix\n    const coOccurrences = buildCoOccurrenceMatrix(entities, allArticles);\n\n    // Calculate PMI scores for all entity pairs\n    const relationships = [];\n\n    for (let i = 0; i < entities.length; i++) {\n      for (let j = i + 1; j < entities.length; j++) {\n        const entity1 = entities[i];\n        const entity2 = entities[j];\n\n        const pmiScore = calculatePMI(\n          entity1.id,\n          entity2.id,\n          coOccurrences,\n          allArticles.length\n        );\n\n        // Only create relationships above threshold\n        if (pmiScore >= minStrength) {\n          relationships.push({\n            source: entity1,\n            target: entity2,\n            strength: pmiScore,\n            coOccurrences: coOccurrences.get(`${entity1.id}-${entity2.id}`) || 0\n          });\n        }\n      }\n    }\n\n    // Sort by strength and limit\n    relationships.sort((a, b) => b.strength - a.strength);\n    const topRelationships = relationships.slice(0, maxRelationships);\n\n    // Use AI to semantically label relationships\n    if (useSemanticLabeling && topRelationships.length > 0) {\n      await labelRelationshipsWithAI(topRelationships);\n    }\n\n    return topRelationships;\n  } catch (error) {\n    console.error('Relationship discovery failed:', error);\n    return [];\n  }\n}\n\n/**\n * Build co-occurrence matrix from articles\n * Optimized: O(articles × entities²) using Set lookup instead of O(articles × entities³)\n * @param {Array<Object>} entities - All entities\n * @param {Array<Object>} articles - All articles\n * @returns {Map} Co-occurrence counts\n */\nfunction buildCoOccurrenceMatrix(entities, articles) {\n  const coOccurrences = new Map();\n  const entityOccurrences = new Map();\n\n  // Create Set of entity IDs for O(1) lookup instead of O(n) .some()\n  const targetEntityIds = new Set(entities.map(e => e.id));\n\n  // Initialize entity occurrence counts\n  entities.forEach(e => entityOccurrences.set(e.id, 0));\n\n  // Count co-occurrences in articles\n  for (const article of articles) {\n    const articleEntities = article.entities || [];\n\n    // Count individual occurrences\n    articleEntities.forEach(entityId => {\n      if (entityOccurrences.has(entityId)) {\n        entityOccurrences.set(entityId, entityOccurrences.get(entityId) + 1);\n      }\n    });\n\n    // Count co-occurrences (pairs) - O(entities²) per article\n    for (let i = 0; i < articleEntities.length; i++) {\n      for (let j = i + 1; j < articleEntities.length; j++) {\n        const id1 = articleEntities[i];\n        const id2 = articleEntities[j];\n\n        // O(1) Set lookup instead of O(n) .some() - Critical optimization\n        if (targetEntityIds.has(id1) && targetEntityIds.has(id2)) {\n          const key1 = `${id1}-${id2}`;\n          const key2 = `${id2}-${id1}`;\n\n          const currentCount = coOccurrences.get(key1) || 0;\n          coOccurrences.set(key1, currentCount + 1);\n          coOccurrences.set(key2, currentCount + 1);\n        }\n      }\n    }\n  }\n\n  // Store individual occurrence counts for PMI calculation\n  coOccurrences.set('__occurrences__', entityOccurrences);\n\n  return coOccurrences;\n}\n\n/**\n * Calculate PMI (Pointwise Mutual Information) score\n * PMI(x,y) = log(P(x,y) / (P(x) * P(y)))\n * Normalized to [0, 1] range\n * @param {string} entityId1 - First entity ID\n * @param {string} entityId2 - Second entity ID\n * @param {Map} coOccurrences - Co-occurrence data\n * @param {number} totalArticles - Total number of articles\n * @returns {number} PMI score between 0 and 1\n */\nfunction calculatePMI(entityId1, entityId2, coOccurrences, totalArticles) {\n  const occurrences = coOccurrences.get('__occurrences__');\n\n  const count1 = occurrences.get(entityId1) || 0;\n  const count2 = occurrences.get(entityId2) || 0;\n  const coCount = coOccurrences.get(`${entityId1}-${entityId2}`) || 0;\n\n  // Avoid division by zero\n  if (count1 === 0 || count2 === 0 || coCount === 0 || totalArticles === 0) {\n    return 0;\n  }\n\n  // Calculate probabilities\n  const pX = count1 / totalArticles;\n  const pY = count2 / totalArticles;\n  const pXY = coCount / totalArticles;\n\n  // PMI formula: log(P(x,y) / (P(x) * P(y)))\n  const pmi = Math.log2(pXY / (pX * pY));\n\n  // Normalize PMI to [0, 1] range\n  // PMI can be negative (entities occur together less than by chance)\n  // We only care about positive associations, so clip to [0, max]\n  const maxPMI = 10;  // Theoretical maximum is log2(N), use 10 as practical max\n  const normalizedPMI = Math.max(0, Math.min(pmi / maxPMI, 1));\n\n  // Boost score based on co-occurrence frequency\n  // More co-occurrences = more confident relationship\n  const frequencyBoost = Math.min(coCount / 5, 0.2);  // Max 0.2 boost\n\n  return Math.min(normalizedPMI + frequencyBoost, 1);\n}\n\n/**\n * Use AI to semantically label relationships\n * Determines relationship type (e.g., \"founded by\", \"competes with\", \"uses\")\n * @param {Array<Object>} relationships - Relationships to label\n */\nasync function labelRelationshipsWithAI(relationships) {\n  try {\n    // Check if Prompt API is available\n    if (typeof LanguageModel === 'undefined') {\n      // console.log('Prompt API not available, using default labels');\n      labelWithDefaultTypes(relationships);\n      return;\n    }\n\n    // Check availability (LanguageModel.capabilities doesn't exist, use availability)\n    try {\n      const availability = await LanguageModel.availability();\n      if (availability !== 'readily' && availability !== 'available') {\n        labelWithDefaultTypes(relationships);\n        return;\n      }\n    } catch (error) {\n      // If availability check fails, try to proceed anyway\n      console.warn('LanguageModel availability check failed, attempting to use anyway');\n    }\n\n    // Process in batches of 5 to avoid overwhelming the API\n    const batchSize = 5;\n    for (let i = 0; i < relationships.length; i += batchSize) {\n      const batch = relationships.slice(i, i + batchSize);\n      await labelBatchWithAI(batch);\n\n      // Small delay between batches\n      if (i + batchSize < relationships.length) {\n        await new Promise(resolve => setTimeout(resolve, 100));\n      }\n    }\n  } catch (error) {\n    console.error('AI labeling failed, using defaults:', error);\n    labelWithDefaultTypes(relationships);\n  }\n}\n\n/**\n * Label a batch of relationships using AI\n * @param {Array<Object>} batch - Batch of relationships\n */\nasync function labelBatchWithAI(batch) {\n  try {\n    const session = await LanguageModel.create({\n      systemPrompt: 'You classify relationships between entities. Return only JSON.',\n      temperature: 0.3,\n      topK: 40\n    });\n\n    // Build prompt with entity pairs\n    const pairs = batch.map((rel, idx) =>\n      `${idx + 1}. ${rel.source.name} (${rel.source.type}) ↔ ${rel.target.name} (${rel.target.type})`\n    ).join('\\n');\n\n    const prompt = `Classify the relationship type for these entity pairs. Choose from: founded, acquired, invested_in, partnered_with, competes_with, uses, works_at, member_of, related_to.\n\nEntity pairs:\n${pairs}\n\nReturn JSON array with format: [{\"index\": 1, \"type\": \"founded\", \"description\": \"brief description\"}]`;\n\n    const response = await session.prompt(prompt);\n    session.destroy();\n\n    // Parse AI response\n    const cleaned = response.replace(/```json\\n?/g, '').replace(/```\\n?/g, '').trim();\n    const labels = JSON.parse(cleaned);\n\n    // Apply labels to relationships\n    labels.forEach(label => {\n      const idx = label.index - 1;\n      if (idx >= 0 && idx < batch.length) {\n        batch[idx].type = label.type || 'related_to';\n        batch[idx].description = label.description || '';\n      }\n    });\n  } catch (error) {\n    console.error('Batch AI labeling failed:', error);\n    labelWithDefaultTypes(batch);\n  }\n}\n\n/**\n * Apply default relationship types based on entity types\n * @param {Array<Object>} relationships - Relationships to label\n */\nfunction labelWithDefaultTypes(relationships) {\n  for (const rel of relationships) {\n    const type1 = rel.source.type;\n    const type2 = rel.target.type;\n\n    // Infer type from entity types\n    if (type1 === 'person' && type2 === 'company') {\n      rel.type = 'works_at';\n      rel.description = `${rel.source.name} works at ${rel.target.name}`;\n    } else if (type1 === 'company' && type2 === 'company') {\n      rel.type = 'competes_with';\n      rel.description = `${rel.source.name} and ${rel.target.name} are related companies`;\n    } else if (type1 === 'company' && type2 === 'technology') {\n      rel.type = 'uses';\n      rel.description = `${rel.source.name} uses ${rel.target.name}`;\n    } else if (type1 === 'person' && type2 === 'person') {\n      rel.type = 'related_to';\n      rel.description = `${rel.source.name} and ${rel.target.name} are related`;\n    } else {\n      rel.type = 'related_to';\n      rel.description = `Related entities`;\n    }\n  }\n}\n\n/**\n * Store discovered relationships in the database\n * @param {Array<Object>} relationships - Relationships to store\n * @param {Array<string>} sourceArticles - Article IDs where relationships were found\n * @returns {Promise<number>} Number of relationships stored\n */\nexport async function storeRelationships(relationships, allArticles = []) {\n  try {\n    const store = await getGraphStore();\n    let storedCount = 0;\n\n    for (const rel of relationships) {\n      const relationship = createRelationship(\n        rel.source.id,\n        rel.target.id,\n        rel.type || 'related_to',\n        rel.description || '',\n        rel.strength\n      );\n\n      // Find articles where BOTH entities appear together\n      const coOccurringArticles = [];\n      if (Array.isArray(allArticles)) {\n        for (const article of allArticles) {\n          const articleEntities = article.entities || [];\n          if (articleEntities.includes(rel.source.id) && articleEntities.includes(rel.target.id)) {\n            coOccurringArticles.push(article.id);\n          }\n        }\n      }\n\n      relationship.sources = coOccurringArticles;\n      relationship.metadata.coOccurrences = rel.coOccurrences || 1;\n      relationship.metadata.contexts = [];\n\n      await store.addRelationship(relationship);\n      storedCount++;\n    }\n\n    // console.log(`Stored ${storedCount} relationships`);\n    return storedCount;\n  } catch (error) {\n    console.error('Failed to store relationships:', error);\n    return 0;\n  }\n}\n\n/**\n * Discover and store relationships for all entities in the graph\n * Called periodically or after capturing multiple articles\n * @returns {Promise<Object>} Discovery results\n */\nexport async function discoverAllRelationships() {\n  try {\n    // console.log('Starting relationship discovery...');\n\n    const store = await getGraphStore();\n    const settings = await store.getSettings();\n    const allEntities = await store.getAllEntities();\n    const allArticles = await store.getAllArticles();\n\n    if (allEntities.length < 2) {\n      return { success: true, relationshipsFound: 0, message: 'Not enough entities' };\n    }\n\n    // Performance optimization: Scale relationship discovery based on data size\n    let entitiesToAnalyze = allEntities;\n    let maxRelationships = 100;\n\n    // Strategy 1: For >50 articles, only analyze entities in user's topics of interest\n    if (allArticles.length > 50 && settings.topics && settings.topics.length > 0) {\n      const userTopics = settings.topics.filter(t => t && t.trim());\n      if (userTopics.length > 0) {\n        entitiesToAnalyze = allEntities.filter(e =>\n          userTopics.some(topic =>\n            e.topic && e.topic.toLowerCase().includes(topic.toLowerCase())\n          )\n        );\n        maxRelationships = 50;\n        // console.log(`Large dataset (${allArticles.length} articles): Analyzing ${entitiesToAnalyze.length} topic-related entities`);\n      }\n    }\n\n    // Strategy 2: For >20 articles, limit to entities from recent 15 articles\n    else if (allArticles.length > 20) {\n      const recentArticles = allArticles\n        .sort((a, b) => b.capturedAt - a.capturedAt)\n        .slice(0, 15);\n\n      const recentEntityIds = new Set();\n      recentArticles.forEach(article => {\n        (article.entities || []).forEach(id => recentEntityIds.add(id));\n      });\n\n      entitiesToAnalyze = allEntities.filter(e => recentEntityIds.has(e.id));\n      maxRelationships = 75;\n      // console.log(`Medium dataset (${allArticles.length} articles): Analyzing ${entitiesToAnalyze.length} entities from recent 15 articles`);\n    }\n\n    // Ensure we don't analyze too many entities at once (prevent crash)\n    if (entitiesToAnalyze.length > 200) {\n      entitiesToAnalyze = entitiesToAnalyze\n        .sort((a, b) => (b.relevance || 0) - (a.relevance || 0))\n        .slice(0, 200);\n      // console.log(`Limited to top 200 entities by relevance to prevent performance issues`);\n    }\n\n    // Find relationships with PMI + AI labeling (co-occurring entities)\n    const relationships = await findRelationships(entitiesToAnalyze, {\n      minStrength: 0.3,\n      useSemanticLabeling: false, // Disable AI labeling for performance - use default types\n      maxRelationships: maxRelationships\n    });\n\n    // Store relationships with actual articles for proper source attribution\n    const storedCount = await storeRelationships(relationships, allArticles);\n\n    // console.log(`Relationship discovery complete: ${storedCount} relationships found`);\n\n    return {\n      success: true,\n      relationshipsFound: storedCount,\n      totalEntities: allEntities.length,\n      analyzedEntities: entitiesToAnalyze.length,\n      totalArticles: allArticles.length\n    };\n  } catch (error) {\n    console.error('Relationship discovery failed:', error);\n    return { success: false, error: error.message };\n  }\n}\n","// Mock data generator for development and testing\nimport { ENTITY_TYPES, RELATIONSHIP_TYPES } from './constants.js';\n\n/**\n * Generate realistic mock entities for testing\n * @param {number} count - Number of entities to generate\n * @returns {Array<Object>} Array of mock entities\n */\nexport function generateMockEntities(count = 50) {\n  const mockData = {\n    person: [\n      { name: 'Sundar Pichai', topic: 'Chrome AI' },\n      { name: 'Demis Hassabis', topic: 'Chrome AI' },\n      { name: 'Jeff Dean', topic: 'Chrome AI' },\n      { name: 'Parisa Tabriz', topic: 'Chrome Security' },\n      { name: 'Alex Russell', topic: 'Web Platform' },\n      { name: 'Jake Archibald', topic: 'Web Performance' },\n      { name: 'Addy Osmani', topic: 'Chrome DevTools' },\n      { name: 'Paul Irish', topic: 'Chrome DevTools' }\n    ],\n    company: [\n      { name: 'Google', topic: 'Chrome AI' },\n      { name: 'Google DeepMind', topic: 'Chrome AI' },\n      { name: 'Anthropic', topic: 'AI Safety' },\n      { name: 'Google Chrome Team', topic: 'Browser Development' },\n      { name: 'Mozilla', topic: 'Browser Privacy' }\n    ],\n    technology: [\n      { name: 'Gemini Nano', topic: 'Chrome AI' },\n      { name: 'Chrome Prompt API', topic: 'Chrome AI' },\n      { name: 'Chrome Summarizer API', topic: 'Chrome AI' },\n      { name: 'Chrome Rewriter API', topic: 'Chrome AI' },\n      { name: 'Chrome Language Detector', topic: 'Chrome AI' },\n      { name: 'Chrome Writer API', topic: 'Chrome AI' },\n      { name: 'On-device AI', topic: 'Chrome AI' },\n      { name: 'Chrome Extensions', topic: 'Browser Development' },\n      { name: 'Manifest V3', topic: 'Browser Development' },\n      { name: 'Service Workers', topic: 'Web Platform' },\n      { name: 'IndexedDB', topic: 'Web Platform' },\n      { name: 'WebAssembly', topic: 'Web Performance' },\n      { name: 'React', topic: 'Frontend' },\n      { name: 'D3.js', topic: 'Data Visualization' }\n    ],\n    concept: [\n      { name: 'Privacy-first AI', topic: 'Chrome AI' },\n      { name: 'Local-first Software', topic: 'Privacy' },\n      { name: 'On-device Inference', topic: 'Chrome AI' },\n      { name: 'Browser-level Intelligence', topic: 'Chrome AI' },\n      { name: 'Zero-latency AI', topic: 'Performance' },\n      { name: 'Offline-first Applications', topic: 'Web Platform' },\n      { name: 'Edge AI', topic: 'Chrome AI' },\n      { name: 'Knowledge Graphs', topic: 'Data Structures' },\n      { name: 'Entity Extraction', topic: 'NLP' },\n      { name: 'Relationship Discovery', topic: 'Graph Theory' },\n      { name: 'Browser Extensions Security', topic: 'Chrome Security' },\n      { name: 'GDPR Compliance', topic: 'Privacy' }\n    ]\n  };\n\n  const entities = [];\n  const types = Object.keys(mockData);\n  let id = 1;\n\n  // Generate entities from mock data\n  types.forEach(type => {\n    mockData[type].forEach(item => {\n      if (entities.length < count) {\n        entities.push({\n          id: `entity-${id++}`,\n          name: item.name,\n          type,\n          topic: item.topic,\n          relevance: Math.random() * 0.5 + 0.5, // 0.5 to 1.0\n          timestamp: Date.now() - Math.random() * 30 * 24 * 60 * 60 * 1000 // last 30 days\n        });\n      }\n    });\n  });\n\n  return entities.slice(0, count);\n}\n\n/**\n * Generate realistic relationships between entities\n * @param {Array<Object>} entities - Entities to create relationships for\n * @param {number} relationshipDensity - Average relationships per entity (default 2)\n * @returns {Array<Object>} Array of mock relationships\n */\nexport function generateMockRelationships(entities, relationshipDensity = 2) {\n  const relationships = [];\n  const relationshipMap = new Map(); // Track existing relationships\n\n  // Predefined relationship patterns for realism\n  const patterns = [\n    // Person → Company (works at, founded, leads)\n    { sourceType: 'person', targetType: 'company', relType: 'direct', descriptions: ['founded', 'leads', 'works at'] },\n\n    // Person → Technology (created, developed)\n    { sourceType: 'person', targetType: 'technology', relType: 'direct', descriptions: ['created', 'developed', 'pioneered'] },\n\n    // Company → Technology (develops, uses)\n    { sourceType: 'company', targetType: 'technology', relType: 'direct', descriptions: ['develops', 'uses', 'maintains'] },\n\n    // Technology → Concept (implements, enables)\n    { sourceType: 'technology', targetType: 'concept', relType: 'conceptual', descriptions: ['implements', 'enables', 'demonstrates'] },\n\n    // Person → Concept (researches, advocates)\n    { sourceType: 'person', targetType: 'concept', relType: 'conceptual', descriptions: ['researches', 'advocates for', 'specializes in'] },\n\n    // Company → Concept (focuses on, advances)\n    { sourceType: 'company', targetType: 'concept', relType: 'conceptual', descriptions: ['focuses on', 'advances', 'invests in'] }\n  ];\n\n  let relationshipId = 1;\n  const targetRelationshipCount = entities.length * relationshipDensity;\n\n  // Create relationships based on patterns\n  entities.forEach(source => {\n    const applicablePatterns = patterns.filter(p => p.sourceType === source.type);\n\n    applicablePatterns.forEach(pattern => {\n      const targets = entities.filter(e =>\n        e.type === pattern.targetType &&\n        e.id !== source.id &&\n        e.topic === source.topic // Same topic for higher relevance\n      );\n\n      if (targets.length > 0 && relationships.length < targetRelationshipCount) {\n        // Create 1-2 relationships per applicable pattern\n        const numRels = Math.random() > 0.5 ? 1 : 2;\n\n        for (let i = 0; i < numRels && relationships.length < targetRelationshipCount; i++) {\n          const target = targets[Math.floor(Math.random() * targets.length)];\n          const relKey = `${source.id}-${target.id}`;\n          const reverseKey = `${target.id}-${source.id}`;\n\n          // Avoid duplicate relationships\n          if (!relationshipMap.has(relKey) && !relationshipMap.has(reverseKey)) {\n            const description = pattern.descriptions[Math.floor(Math.random() * pattern.descriptions.length)];\n\n            relationships.push({\n              id: `rel-${relationshipId++}`,\n              source: source.id,\n              target: target.id,\n              type: pattern.relType,\n              strength: Math.random() * 0.5 + 0.5, // 0.5 to 1.0\n              description: `${source.name} ${description} ${target.name}`,\n              timestamp: Date.now()\n            });\n\n            relationshipMap.set(relKey, true);\n          }\n        }\n      }\n    });\n  });\n\n  // Add some cross-topic relationships for interesting connections\n  const crossTopicCount = Math.floor(entities.length * 0.3);\n  for (let i = 0; i < crossTopicCount && relationships.length < targetRelationshipCount * 1.2; i++) {\n    const source = entities[Math.floor(Math.random() * entities.length)];\n    const target = entities[Math.floor(Math.random() * entities.length)];\n\n    if (source.id !== target.id && source.topic !== target.topic) {\n      const relKey = `${source.id}-${target.id}`;\n      const reverseKey = `${target.id}-${source.id}`;\n\n      if (!relationshipMap.has(relKey) && !relationshipMap.has(reverseKey)) {\n        relationships.push({\n          id: `rel-${relationshipId++}`,\n          source: source.id,\n          target: target.id,\n          type: RELATIONSHIP_TYPES.CONCEPTUAL,\n          strength: Math.random() * 0.3 + 0.3, // Lower strength for cross-topic\n          description: `${source.name} relates to ${target.name}`,\n          timestamp: Date.now()\n        });\n\n        relationshipMap.set(relKey, true);\n      }\n    }\n  }\n\n  return relationships;\n}\n\n/**\n * Populate IndexedDB with mock data for testing\n * @param {Object} store - Graph store instance\n * @param {number} entityCount - Number of entities to generate\n * @returns {Promise<Object>} Stats about generated data\n */\nexport async function populateMockData(store, entityCount = 50) {\n  const entities = generateMockEntities(entityCount);\n  const relationships = generateMockRelationships(entities, 2);\n\n  // Clear existing data\n  await store.clearAll();\n\n  // Add entities\n  for (const entity of entities) {\n    await store.addEntity(entity);\n  }\n\n  // Add relationships\n  for (const relationship of relationships) {\n    await store.addRelationship(relationship);\n  }\n\n  return {\n    entitiesAdded: entities.length,\n    relationshipsAdded: relationships.length,\n    avgDegree: (relationships.length * 2 / entities.length).toFixed(2)\n  };\n}\n\n/**\n * Generate mock articles for testing article capture\n * @param {number} count - Number of articles\n * @returns {Array<Object>} Mock articles\n */\nexport function generateMockArticles(count = 5) {\n  const articles = [\n    {\n      title: 'Chrome Built-in AI: Gemini Nano Powers On-Device Intelligence',\n      url: 'https://developer.chrome.com/docs/ai/built-in-apis',\n      content: 'Google Chrome introduces Gemini Nano for on-device AI processing. The Chrome Prompt API, Summarizer API, and Rewriter API enable privacy-first intelligence directly in the browser. Sundar Pichai emphasizes zero-latency inference and GDPR compliance through local-first architecture.',\n      timestamp: Date.now() - 1 * 24 * 60 * 60 * 1000\n    },\n    {\n      title: 'Building Privacy-First Chrome Extensions with Manifest V3',\n      url: 'https://developer.chrome.com/docs/extensions/mv3',\n      content: 'Google Chrome Team releases Manifest V3 specifications for browser extensions. The new service worker architecture and IndexedDB integration enable powerful offline-first applications while maintaining browser extensions security. Key benefits include enhanced privacy and performance.',\n      timestamp: Date.now() - 3 * 24 * 60 * 60 * 1000\n    },\n    {\n      title: 'Local-First Software: Seven Ideals for Distributed Systems',\n      url: 'https://example.com/local-first-software',\n      content: 'Research paper explores local-first software principles, emphasizing on-device inference and offline-first applications. The paper discusses how IndexedDB and Service Workers enable edge AI without cloud dependencies, making browser-level intelligence a reality.',\n      timestamp: Date.now() - 5 * 24 * 60 * 60 * 1000\n    },\n    {\n      title: 'WebAssembly Performance Benchmarks for AI Workloads',\n      url: 'https://example.com/wasm-ai-performance',\n      content: 'New benchmarks show WebAssembly achieving near-native performance for on-device AI inference. The integration with Chrome Extensions and React frontends enables sophisticated knowledge graph applications with D3.js visualizations running at 60fps.',\n      timestamp: Date.now() - 7 * 24 * 60 * 60 * 1000\n    },\n    {\n      title: 'Chrome AI Challenge 2025: Building the Future of Intelligent Browsers',\n      url: 'https://googlechromeai2025.devpost.com',\n      content: 'Google announces the Chrome AI Challenge 2025, inviting developers to build intelligent browser extensions using Gemini Nano. The competition showcases browser-level intelligence, entity extraction, and knowledge graph applications powered by the Chrome Prompt API and on-device AI.',\n      timestamp: Date.now() - 9 * 24 * 60 * 60 * 1000\n    }\n  ];\n\n  return articles.slice(0, count);\n}\n\nexport default {\n  generateMockEntities,\n  generateMockRelationships,\n  populateMockData,\n  generateMockArticles\n};\n","/**\n * Test script for Chrome Writer API quality assessment\n *\n * This tests whether the Writer API can generate genuinely insightful\n * weekly learning summaries, or if it produces generic output.\n *\n * Decision criteria: Output must be \"I would personally want this\" quality.\n */\n\n/**\n * Test the Writer API with sample article data\n * @param {Array} articles - Sample articles from last 7 days\n * @param {Array} topEntities - Top 15 most frequent entities\n * @returns {Promise<{quality: string, output: string, recommendation: string}>}\n */\nexport async function testWriterQuality(articles, topEntities) {\n  try {\n    // Check if Writer API is available\n    if (!window.ai || !window.ai.writer) {\n      return {\n        quality: 'unavailable',\n        output: null,\n        recommendation: 'Skip - API not available'\n      };\n    }\n\n    const { available } = await window.ai.writer.capabilities();\n    if (available === 'no') {\n      return {\n        quality: 'unavailable',\n        output: null,\n        recommendation: 'Skip - API not ready'\n      };\n    }\n\n    // Create writer session\n    const writer = await window.ai.writer.create({\n      sharedContext: `You are a world-class research analyst. Your job is to synthesize a user's reading history into a concise, insightful narrative. Identify the 2-3 main themes and show how they connect. Do not just list the articles. Focus on emerging patterns, connections between concepts, and intellectual trajectory.`\n    });\n\n    // Build prompt from sample data\n    const articleTitles = articles.map(a => `- ${a.title}`).join('\\n');\n    const entityList = topEntities.map(e => e.name).join(', ');\n\n    const prompt = `Synthesize this week's learning into a narrative paragraph (150-200 words).\n\nArticles captured:\n${articleTitles}\n\nKey concepts explored: ${entityList}\n\nWrite a synthesis that:\n1. Identifies the 2-3 main themes\n2. Shows how concepts connect across articles\n3. Reveals the intellectual trajectory\n4. Sounds insightful, not generic`;\n\n    const output = await writer.write(prompt);\n\n    // Quality assessment heuristics\n    const quality = assessOutputQuality(output, articles, topEntities);\n\n    return {\n      quality,\n      output,\n      recommendation: quality === 'exceptional' || quality === 'good'\n        ? 'Build it - output is valuable'\n        : 'Skip - output too generic'\n    };\n\n  } catch (error) {\n    console.error('Writer API test failed:', error);\n    return {\n      quality: 'error',\n      output: null,\n      recommendation: 'Skip - API unstable'\n    };\n  }\n}\n\n/**\n * Assess quality of Writer API output\n * @param {string} output - Generated text\n * @param {Array} articles - Input articles\n * @param {Array} entities - Input entities\n * @returns {string} - 'exceptional', 'good', 'mediocre', 'poor'\n */\nfunction assessOutputQuality(output, articles, entities) {\n  // Red flags (generic/poor output)\n  const genericPhrases = [\n    'you read about',\n    'you explored',\n    'this week you focused on',\n    'various topics',\n    'different aspects'\n  ];\n\n  const hasGenericLanguage = genericPhrases.some(phrase =>\n    output.toLowerCase().includes(phrase)\n  );\n\n  // Good signs (insightful output)\n  const connectiveWords = ['connects', 'relationship', 'trade-off', 'bridge', 'thread', 'tension', 'emerges'];\n  const hasConnectiveLanguage = connectiveWords.some(word =>\n    output.toLowerCase().includes(word)\n  );\n\n  // Check if it just lists entities\n  const entityCount = entities.filter(e =>\n    output.includes(e.name)\n  ).length;\n  const justListingEntities = entityCount > entities.length * 0.7;\n\n  // Check for narrative depth (compound sentences)\n  const sentences = output.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  const avgWordsPerSentence = output.split(/\\s+/).length / sentences.length;\n  const hasDepth = avgWordsPerSentence > 15; // Longer sentences = more complex ideas\n\n  // Scoring\n  if (hasConnectiveLanguage && hasDepth && !hasGenericLanguage && !justListingEntities) {\n    return 'exceptional';\n  } else if (hasConnectiveLanguage && !hasGenericLanguage) {\n    return 'good';\n  } else if (hasGenericLanguage || justListingEntities) {\n    return 'poor';\n  } else {\n    return 'mediocre';\n  }\n}\n\n/**\n * Generate sample data for testing\n * @returns {Object} - {articles, topEntities}\n */\nexport function generateSampleData() {\n  return {\n    articles: [\n      {\n        title: 'Chrome Built-in AI: Gemini Nano Performance Benchmarks',\n        capturedAt: Date.now() - 86400000 * 1\n      },\n      {\n        title: 'Privacy Implications of On-Device Machine Learning',\n        capturedAt: Date.now() - 86400000 * 2\n      },\n      {\n        title: 'WebAssembly for High-Performance Browser Extensions',\n        capturedAt: Date.now() - 86400000 * 3\n      },\n      {\n        title: 'Local-First Software: Seven Ideals for Distributed Systems',\n        capturedAt: Date.now() - 86400000 * 4\n      },\n      {\n        title: 'The Future of Browser APIs: AI Integration Patterns',\n        capturedAt: Date.now() - 86400000 * 5\n      }\n    ],\n    topEntities: [\n      { name: 'Gemini Nano', frequency: 12 },\n      { name: 'Chrome Extensions', frequency: 10 },\n      { name: 'On-device AI', frequency: 9 },\n      { name: 'Privacy', frequency: 8 },\n      { name: 'WebAssembly', frequency: 7 },\n      { name: 'Performance', frequency: 6 },\n      { name: 'Local-first', frequency: 5 },\n      { name: 'Browser APIs', frequency: 5 },\n      { name: 'Machine Learning', frequency: 4 },\n      { name: 'Distributed Systems', frequency: 4 }\n    ]\n  };\n}\n","import { getGraphStore } from '../storage/graph-store.js';\nimport { getGraphEngine } from '../graph/graph-engine.js';\nimport { extractEntitiesWithSummarization } from '../graph/entity-extractor.js';\nimport { discoverAllRelationships } from '../graph/relationship-finder.js';\nimport { checkAllAIAvailability } from '../utils/availability.js';\nimport { createEntity, createArticle } from '../utils/helpers.js';\nimport { populateMockData } from '../utils/mock-data.js';\nimport { testWriterQuality, generateSampleData } from '../utils/writer-test.js';\nimport { promptOnce } from '../utils/ai-helpers.js';\n\n// Initialize extension on install/startup\nchrome.runtime.onInstalled.addListener(() => {\n  initializeExtension();\n  setupContextMenus();\n});\n\nasync function initializeExtension() {\n  try {\n    const store = await getGraphStore();\n    const settings = await store.getSettings();\n    if (!settings.id) {\n      await store.updateSettings(settings);\n    }\n\n    await checkAllAIAvailability();\n  } catch (error) {\n    console.error('Initialization failed:', error);\n  }\n}\n\n// Setup context menus for right-click capture\nfunction setupContextMenus() {\n  chrome.contextMenus.create({\n    id: 'captureArticle',\n    title: 'Capture Insights with Glyph',\n    contexts: ['page']\n  });\n\n\n  chrome.contextMenus.create({\n    id: 'captureAllTabs',\n    title: 'Capture All Tabs',\n    contexts: ['page']\n  });\n}\n\n// Handle context menu clicks\nchrome.contextMenus.onClicked.addListener(async (info, tab) => {\n  if (info.menuItemId === 'captureArticle') {\n    captureCurrentTab(tab);\n  } else if (info.menuItemId === 'captureAllTabs') {\n    captureAllOpenTabs();\n  }\n});\n\n// Badge removed - ambient intelligence, not notifications\n\n// Message handler for communication with popup and content scripts\nchrome.runtime.onMessage.addListener((message, sender, sendResponse) => {\n  handleMessage(message, sender)\n    .then(response => sendResponse(response))\n    .catch(error => {\n      console.error('Message handler error:', error);\n      sendResponse({ success: false, error: error.message });\n    });\n\n  return true; // Keep channel open for async\n});\n\nasync function handleMessage(message, sender) {\n  switch (message.type) {\n    case 'GET_GRAPH_DATA':\n      return await getGraphData();\n\n    case 'CHECK_DUPLICATE':\n      return await checkForDuplicate(message.data);\n\n    case 'CAPTURE_ARTICLE':\n      return await captureArticle(message.data);\n\n    case 'GET_SETTINGS':\n      return await getSettings();\n\n    case 'UPDATE_SETTINGS':\n      return await updateSettings(message.data);\n\n    case 'CHECK_AI_STATUS':\n      return await checkAIAvailability();\n\n    case 'GET_STATISTICS':\n      return await getStatistics();\n\n    case 'LOAD_MOCK_DATA':\n      return await loadMockData(message.entityCount);\n\n    case 'QUERY_GRAPH':\n      return await queryGraph(message.query, message.graphData);\n\n    case 'QUERY_KNOWLEDGE':\n      return await queryKnowledge(message.query);\n\n    case 'ANSWER_ENTITY_QUESTION':\n      return await answerEntityQuestion(message.entityId, message.questionType);\n\n    case 'GENERATE_AI_QUESTION':\n      return await generateAiResearchQuestion(message.prompt);\n\n    case 'CAPTURE_CURRENT_TAB':\n      return await captureCurrentTab(message.tab);\n\n    case 'CAPTURE_ALL_TABS':\n      return await captureAllOpenTabs();\n\n    case 'DISCOVER_RELATIONSHIPS':\n      return await discoverAllRelationships();\n\n    case 'GET_ALL_ARTICLES':\n      return await getAllArticles();\n\n\n    case 'DELETE_ARTICLE':\n      return await deleteArticle(message.articleId);\n\n    case 'CLEAR_ALL_DATA':\n      return await clearAllData();\n\n    case 'TEST_WRITER_API':\n      return await testWriterAPI();\n\n    case 'SYNTHESIZE_WEEK':\n      synthesizeWeekBackground(message.weekStart, message.force);\n      return {\n        success: true,\n        message: 'Synthesis started in background. Check back in a few moments.'\n      };\n\n    case 'GET_ALL_SYNTHESES':\n      return await getAllSyntheses();\n\n    case 'GET_LATEST_SYNTHESIS':\n      return await getLatestSynthesis();\n\n    case 'ANALYZE_PAGE_FOR_INSIGHTS':\n      return await analyzePageForInsights(message.data);\n\n    case 'OPEN_GRAPH_PAGE':\n      return await openGraphPage(message.articleId);\n\n    default:\n      throw new Error(`Unknown message type: ${message.type}`);\n  }\n}\n\nasync function getGraphData() {\n  try {\n    const store = await getGraphStore();\n    const entities = await store.getAllEntities();\n    const relationships = await store.getAllRelationships();\n\n    const nodes = entities.map(entity => ({\n      id: entity.id,\n      name: entity.name,\n      type: entity.type,\n      topic: entity.topic,\n      relevance: entity.relevance\n    }));\n\n    const links = relationships.map(rel => ({\n      id: rel.id,\n      source: rel.source,\n      target: rel.target,\n      type: rel.type,\n      strength: rel.strength,\n      description: rel.description\n    }));\n\n    return { success: true, data: { nodes, links } };\n  } catch (error) {\n    console.error('Failed to get graph data:', error);\n    throw error;\n  }\n}\n\n// URL-specific capture locks to prevent duplicate processing while allowing parallel captures\nconst captureQueue = new Set();\n\nasync function captureArticle(data) {\n  try {\n    // Check if this specific URL is already being processed\n    if (captureQueue.has(data.url)) {\n      console.log(`[DEBUG] 🚫 CAPTURE BLOCKED - This URL is already being processed: ${data.url}`);\n      return { success: false, error: 'This article is already being processed' };\n    }\n\n    // Add URL to processing queue (allows parallel processing of different URLs)\n    captureQueue.add(data.url);\n    console.log(`[DEBUG] 🔒 CAPTURE STARTED for: ${data.url} (${captureQueue.size} total captures running)`);\n\n    const store = await getGraphStore();\n    const settings = await store.getSettings();\n    const userTopics = settings.topics || [];\n\n    // Skip duplicate checking here - popup already handled it with user confirmation\n    console.log(`[DEBUG] Processing article (duplicate check handled by popup): ${data.url}`);\n\n    notifyProcessingStatus('Analyzing article...', true);\n\n    // Extract entities with AI\n    notifyProcessingStatus('Extracting entities with AI...', true);\n    const extractedEntities = await extractEntitiesWithSummarization(\n      data.text,\n      userTopics\n    );\n\n    console.log(`[DEBUG] 🤖 AI extracted ${extractedEntities.length} entities from article`);\n\n    // Create article record\n    const articleId = generateTempUUID();\n    const article = createArticle(\n      data.url,\n      data.title,\n      data.text,\n      '', // Summary will be added later if needed\n      data.language || 'en',\n      []\n    );\n    article.id = articleId;\n\n    // console.log(`Extracted ${extractedEntities.length} entities:`, extractedEntities);\n\n    // Store entities and link to article\n    notifyProcessingStatus('Storing entities...', true);\n    const entityIds = [];\n\n    for (const entityData of extractedEntities) {\n      const entity = createEntity(\n        entityData.name,\n        entityData.type,\n        entityData.topic || userTopics[0] || 'General',\n        entityData.context\n      );\n      entity.sources = [articleId];\n      entity.relevance = entityData.relevance || 0.5;\n\n      try {\n        const storedId = await store.addEntity(entity);\n        entityIds.push(storedId || entity.id);\n      } catch (error) {\n        console.error('Failed to store entity:', entity.name, error);\n      }\n    }\n\n    // Update article with entity references\n    article.entities = entityIds;\n    await store.addArticle(article);\n\n    // Reload graph engine cache\n    notifyProcessingStatus('Updating knowledge graph...', true);\n    const engine = await getGraphEngine();\n    await engine.loadGraph();\n\n    // Discover relationships automatically after each article\n    const allArticles = await store.getAllArticles();\n    if (allArticles.length >= 2) {\n      // Run relationship discovery in background after capture completes to not block\n      setTimeout(async () => {\n        try {\n          await discoverAllRelationships();\n          await engine.reloadGraph();\n          chrome.runtime.sendMessage({ type: 'GRAPH_UPDATED' }).catch(() => {});\n        } catch (error) {\n          console.error('Background relationship discovery failed:', error);\n        }\n      }, 1000); // 1 second delay\n    }\n\n    // Notify popup to refresh\n    chrome.runtime.sendMessage({ type: 'GRAPH_UPDATED' }).catch(() => {});\n\n    notifyProcessingStatus('Article captured successfully!', false);\n\n    // Cleanup: remove URL from processing queue\n    captureQueue.delete(data.url);\n    console.log(`[DEBUG] 🔓 CAPTURE COMPLETED for: ${data.url} (${captureQueue.size} captures still running)`);\n\n    return {\n      success: true,\n      articleId: article.id,\n      entityCount: extractedEntities.length,\n      entities: extractedEntities.map(e => e.name)\n    };\n  } catch (error) {\n    console.error('Article capture failed:', error);\n    notifyProcessingStatus(`Error: ${error.message}`, false);\n\n    // Cleanup: remove URL from processing queue even on error\n    captureQueue.delete(data.url);\n    console.log(`[DEBUG] 🔓 CAPTURE FAILED for: ${data.url} - ${error.message} (${captureQueue.size} captures still running)`);\n\n    throw error;\n  }\n}\n\nasync function getSettings() {\n  try {\n    const store = await getGraphStore();\n    const settings = await store.getSettings();\n    return { success: true, settings };\n  } catch (error) {\n    console.error('Failed to get settings:', error);\n    throw error;\n  }\n}\n\nasync function updateSettings(newSettings) {\n  try {\n    const store = await getGraphStore();\n    await store.updateSettings(newSettings);\n    return { success: true };\n  } catch (error) {\n    console.error('Failed to update settings:', error);\n    throw error;\n  }\n}\n\nasync function checkAIAvailability() {\n  try {\n    const status = await checkAllAIAvailability();\n    return { success: true, status };\n  } catch (error) {\n    console.error('Error checking AI availability:', error);\n    return {\n      success: false,\n      error: error.message,\n      status: {\n        prompt: { available: false, status: 'error' },\n        summarizer: { available: false, status: 'error' },\n        languageDetector: { available: false, status: 'error' }\n      }\n    };\n  }\n}\n\nasync function getStatistics() {\n  try {\n    const store = await getGraphStore();\n    const stats = await store.getStatistics();\n    return { success: true, statistics: stats };\n  } catch (error) {\n    console.error('Failed to get statistics:', error);\n    throw error;\n  }\n}\n\nasync function getAllArticles() {\n  try {\n    const store = await getGraphStore();\n    const articles = await store.getAllArticles();\n    return { success: true, articles };\n  } catch (error) {\n    console.error('Failed to get articles:', error);\n    throw error;\n  }\n}\n\n\nasync function deleteArticle(articleId) {\n  try {\n    const store = await getGraphStore();\n\n    // Get article to find its entities\n    const article = await store.getArticle(articleId);\n    if (!article) {\n      throw new Error('Article not found');\n    }\n\n    // Delete the article\n    await store.db.delete('articles', articleId);\n\n    // Clean up orphaned entities (entities only linked to this article)\n    const allArticles = await store.getAllArticles();\n    for (const entityId of article.entities || []) {\n      const entity = await store.getEntity(entityId);\n      if (entity && entity.sources.length === 1 && entity.sources[0] === articleId) {\n        // This entity is only linked to the deleted article\n        await store.db.delete('entities', entityId);\n\n        // Clean up relationships involving this entity\n        const relationships = await store.getAllRelationships();\n        for (const rel of relationships) {\n          if (rel.source === entityId || rel.target === entityId) {\n            await store.db.delete('relationships', rel.id);\n          }\n        }\n      }\n    }\n\n    // Reload graph engine\n    const engine = await getGraphEngine();\n    await engine.reloadGraph();\n\n    // Update badge\n    await \n    // Notify UI\n    chrome.runtime.sendMessage({ type: 'GRAPH_UPDATED' }).catch(() => {});\n\n    return { success: true };\n  } catch (error) {\n    console.error('Failed to delete article:', error);\n    throw error;\n  }\n}\n\nasync function clearAllData() {\n  try {\n    const store = await getGraphStore();\n    await store.clearAllData();\n\n    // Reload graph engine\n    const engine = await getGraphEngine();\n    await engine.reloadGraph();\n\n    // Update badge\n    await \n    // Notify UI\n    chrome.runtime.sendMessage({ type: 'GRAPH_UPDATED' }).catch(() => {});\n\n    return { success: true };\n  } catch (error) {\n    console.error('Failed to clear data:', error);\n    throw error;\n  }\n}\n\nfunction notifyProcessingStatus(status, processing) {\n  chrome.runtime.sendMessage({\n    type: 'PROCESSING_STATUS',\n    status: status,\n    processing: processing\n  }).catch(() => {\n    // Popup might not be open, ignore error\n  });\n}\n\nfunction generateTempUUID() {\n  return `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n}\n\n\nasync function loadMockData(entityCount = 50) {\n  try {\n    notifyProcessingStatus('Loading mock data...', true);\n\n    const store = await getGraphStore();\n    const stats = await populateMockData(store, entityCount);\n\n    // Reload graph engine\n    const engine = await getGraphEngine();\n    await engine.reloadGraph();\n\n    // Notify popup to refresh\n    chrome.runtime.sendMessage({ type: 'GRAPH_UPDATED' }).catch(() => {});\n\n    notifyProcessingStatus('Mock data loaded successfully!', false);\n\n    return {\n      success: true,\n      stats\n    };\n  } catch (error) {\n    console.error('Failed to load mock data:', error);\n    notifyProcessingStatus(`Error loading mock data: ${error.message}`, false);\n    throw error;\n  }\n}\n// Additional service worker functions for new features\n// This will be appended to service-worker.js\n\n// Capture current tab (context menu or icon click)\nasync function captureCurrentTab(tab) {\n  try {\n    // Simple processing indicator - just show something is happening\n    chrome.action.setBadgeText({ text: '...' });\n    chrome.action.setBadgeBackgroundColor({ color: '#FFA500' }); // Orange\n\n    // Update capture status for popup\n    await chrome.storage.local.set({\n      capturingStatus: {\n        active: true,\n        progress: 'Extracting article content...'\n      }\n    });\n\n    // Notify popup that processing started\n    chrome.runtime.sendMessage({\n      type: 'PROCESSING_STATUS',\n      status: 'Extracting article content...',\n      processing: true\n    }).catch(() => {});\n\n    // Notify extension page that article processing started\n    chrome.runtime.sendMessage({\n      type: 'ARTICLE_PROCESSING_STARTED',\n      article: {\n        url: tab.url,\n        title: 'Processing...',\n        processing: true,\n        timestamp: Date.now()\n      }\n    }).catch(() => {});\n\n    // Inject content script if needed and get article content\n    const results = await chrome.scripting.executeScript({\n      target: { tabId: tab.id },\n      func: extractArticleContent\n    });\n\n    const articleData = results[0]?.result;\n\n    if (!articleData || !articleData.text) {\n      throw new Error('No article content found on this page');\n    }\n\n    // Update popup status\n    chrome.runtime.sendMessage({\n      type: 'PROCESSING_STATUS',\n      status: `Extracting entities... (${articleData.text.length} characters)`,\n      processing: true\n    }).catch(() => {});\n\n    // Update processing status with actual article title\n    chrome.runtime.sendMessage({\n      type: 'ARTICLE_PROCESSING_UPDATED',\n      article: {\n        url: tab.url,\n        title: articleData.title,\n        processing: true,\n        timestamp: Date.now()\n      }\n    }).catch(() => {});\n\n    // Update progress\n    await chrome.storage.local.set({\n      capturingStatus: {\n        active: true,\n        progress: `Extracting entities... (${articleData.text.length} characters)`\n      }\n    });\n\n    // Capture article (this will run in background)\n    const result = await captureArticle(articleData);\n\n    // Reset badge to normal (clear processing indicator)\n    chrome.action.setBadgeText({ text: '' });\n\n    // Clear capturing status\n    await chrome.storage.local.set({\n      capturingStatus: { active: false }\n    });\n\n    // Notify popup that processing completed\n    chrome.runtime.sendMessage({\n      type: 'PROCESSING_STATUS',\n      status: `Complete! Extracted ${result.entityCount} entities`,\n      processing: false\n    }).catch(() => {});\n\n    // Notify extension page that processing completed\n    chrome.runtime.sendMessage({\n      type: 'ARTICLE_PROCESSING_COMPLETE',\n      article: {\n        url: tab.url,\n        title: articleData.title,\n        processing: false,\n        entities: result.entityCount\n      }\n    }).catch(() => {});\n\n    // Notify popup of completion\n    chrome.runtime.sendMessage({\n      type: 'CAPTURE_COMPLETE',\n      entities: result.entityCount,\n      total: (await getStatistics()).statistics.totalEntities\n    }).catch(() => {});\n\n    // Notify extension page to refresh\n    chrome.runtime.sendMessage({\n      type: 'GRAPH_UPDATED'\n    }).catch(() => {});\n\n    return result;\n  } catch (error) {\n    console.error('Capture failed:', error);\n\n    // Reset badge to normal (clear processing indicator)\n    chrome.action.setBadgeText({ text: '' });\n\n    // Notify popup of error\n    chrome.runtime.sendMessage({\n      type: 'PROCESSING_STATUS',\n      status: `Failed: ${error.message}`,\n      processing: false\n    }).catch(() => {});\n\n    // Clear capturing status\n    await chrome.storage.local.set({\n      capturingStatus: { active: false }\n    });\n\n    throw error;\n  }\n}\n\n// Function injected into page to extract article content\nfunction extractArticleContent() {\n  // Try to find article content\n  const selectors = [\n    'article',\n    '[role=\"article\"]',\n    'main article',\n    '.article-content',\n    '.post-content',\n    '.entry-content'\n  ];\n\n  let article = null;\n  for (const selector of selectors) {\n    article = document.querySelector(selector);\n    if (article) break;\n  }\n\n  // Fallback to body if no article found\n  const content = article || document.body;\n\n  // Extract text\n  const title = document.title;\n  const text = content.innerText || content.textContent;\n\n  return {\n    url: window.location.href,\n    title: title,\n    text: text.trim(),\n    language: document.documentElement.lang || 'en'\n  };\n}\n\n// Removed complex icon indicator logic - keeping it simple\n\n// Get current active tab\nasync function getCurrentTab() {\n  const tabs = await chrome.tabs.query({ active: true, currentWindow: true });\n  return tabs[0];\n}\n\n\n// Capture all open tabs\nasync function captureAllOpenTabs() {\n  try {\n    const tabs = await chrome.tabs.query({ currentWindow: true });\n\n    let successCount = 0;\n    let totalEntities = 0;\n\n    for (const tab of tabs) {\n      try {\n        // Skip chrome:// and extension pages\n        if (tab.url.startsWith('chrome://') || tab.url.startsWith('chrome-extension://')) {\n          continue;\n        }\n\n        const result = await captureCurrentTab(tab);\n        successCount++;\n        totalEntities += result.entityCount;\n\n        // Small delay between captures\n        await new Promise(resolve => setTimeout(resolve, 500));\n      } catch (error) {\n        console.error(`Failed to capture tab ${tab.url}:`, error);\n      }\n    }\n\n    chrome.notifications.create({\n      type: 'basic',\n      iconUrl: '/icons/icon128.ico',\n      title: 'Bulk Capture Complete',\n      message: `Captured ${successCount} tabs, extracted ${totalEntities} entities`\n    });\n\n    return { success: true, tabCount: successCount, entityCount: totalEntities };\n  } catch (error) {\n    console.error('Bulk capture failed:', error);\n    throw error;\n  }\n}\n\n// Handle graph queries for chat feature\nasync function queryGraph(query, graphData) {\n  try {\n    // Build context from graph\n    const context = buildQueryContext(query, graphData);\n\n    // Use Prompt API to answer\n    if (typeof LanguageModel === 'undefined') {\n      throw new Error('Prompt API not available');\n    }\n\n    const session = await LanguageModel.create({\n      systemPrompt: 'You answer questions about a knowledge graph. Be concise and factual.',\n      temperature: 0.5,\n      topK: 40\n    });\n\n    const prompt = `Knowledge graph context:\n${context}\n\nQuestion: ${query}\n\nAnswer in 2-3 sentences based only on the context provided.`;\n\n    const answer = await session.prompt(prompt);\n    session.destroy();\n\n    // Polish with Rewriter if available\n    let polished = answer;\n    if (typeof Rewriter !== 'undefined') {\n      try {\n        const rewriter = await Rewriter.create({\n          tone: 'neutral',\n          length: 'as-is'\n        });\n        polished = await rewriter.rewrite(answer);\n        rewriter.destroy();\n      } catch (error) {\n        // console.log('Rewriter not available, using raw response');\n      }\n    }\n\n    return { success: true, answer: polished };\n  } catch (error) {\n    console.error('Query failed:', error);\n    return { success: false, error: error.message };\n  }\n}\n\n// Query knowledge base with full database access\nasync function queryKnowledge(query) {\n  try {\n    const store = await getGraphStore();\n    const entities = await store.getAllEntities();\n    const relationships = await store.getAllRelationships();\n    const articles = await store.getAllArticles();\n\n    // Build comprehensive context\n    const queryLower = query.toLowerCase();\n    const relevantEntities = entities.filter(e =>\n      queryLower.includes(e.name.toLowerCase()) ||\n      e.name.toLowerCase().includes(queryLower) ||\n      e.context?.toLowerCase().includes(queryLower)\n    );\n\n    let context = `Knowledge Base Summary:\\n`;\n    context += `Total: ${entities.length} entities, ${relationships.length} relationships, ${articles.length} articles\\n\\n`;\n\n    if (relevantEntities.length > 0) {\n      context += `Relevant Entities:\\n`;\n      for (const entity of relevantEntities.slice(0, 10)) {\n        context += `- ${entity.name} (${entity.type})\\n`;\n        if (entity.context) context += `  Context: ${entity.context.slice(0, 100)}\\n`;\n\n        // Find relationships\n        const rels = relationships.filter(r =>\n          r.source === entity.id || r.target === entity.id\n        );\n\n        if (rels.length > 0) {\n          const connectedNames = rels.slice(0, 3).map(r => {\n            const otherId = r.source === entity.id ? r.target : r.source;\n            const other = entities.find(e => e.id === otherId);\n            return other ? other.name : null;\n          }).filter(Boolean);\n\n          if (connectedNames.length > 0) {\n            context += `  Connected to: ${connectedNames.join(', ')}\\n`;\n          }\n        }\n      }\n    } else {\n      // If no direct matches, provide general info about all entities\n      const entitySummary = entities.slice(0, 20).map(e => e.name).join(', ');\n      context += `\\nAvailable entities: ${entitySummary}`;\n    }\n\n    // Use Prompt API to answer\n    if (typeof LanguageModel === 'undefined') {\n      throw new Error('Prompt API not available');\n    }\n\n    const session = await LanguageModel.create({\n      systemPrompt: 'You answer questions about a knowledge graph. Be concise and helpful.',\n      temperature: 0.5,\n      topK: 40\n    });\n\n    const prompt = `${context}\\n\\nQuestion: ${query}\\n\\nAnswer based on the knowledge base:`;\n\n    const answer = await session.prompt(prompt);\n    session.destroy();\n\n    return { success: true, answer };\n  } catch (error) {\n    console.error('Knowledge query failed:', error);\n    return { success: false, error: error.message };\n  }\n}\n\n// Answer entity-specific questions (no AI needed)\nasync function answerEntityQuestion(entityId, questionType) {\n  try {\n    const store = await getGraphStore();\n    const entity = await store.getEntity(entityId);\n\n    if (!entity) {\n      return { success: false, error: 'Entity not found' };\n    }\n\n    const [type, id] = questionType.split(':');\n\n    if (type === 'SOURCES') {\n      // Get articles that mention this entity\n      const articles = await store.getAllArticles();\n      const sourceArticles = articles.filter(a =>\n        a.entities && a.entities.includes(entityId)\n      );\n\n      if (sourceArticles.length === 0) {\n        return {\n          success: true,\n          answer: `${entity.name} hasn't been mentioned in any captured articles yet.`\n        };\n      }\n\n      const articleList = sourceArticles\n        .map(a => `• ${a.title}`)\n        .join('\\n');\n\n      return {\n        success: true,\n        answer: `${entity.name} is mentioned in ${sourceArticles.length} article${sourceArticles.length !== 1 ? 's' : ''}:\\n\\n${articleList}`\n      };\n    }\n\n    if (type === 'CONNECTIONS') {\n      // Get connected entities\n      const relationships = await store.getAllRelationships();\n      const allEntities = await store.getAllEntities();\n\n      const connectedRels = relationships.filter(r =>\n        r.source === entityId || r.target === entityId\n      );\n\n      if (connectedRels.length === 0) {\n        return {\n          success: true,\n          answer: `${entity.name} doesn't have any connections yet. Capture more articles to discover relationships.`\n        };\n      }\n\n      const connections = connectedRels.map(r => {\n        const otherId = r.source === entityId ? r.target : r.source;\n        const other = allEntities.find(e => e.id === otherId);\n        if (!other) return null;\n\n        const relType = r.type || 'related to';\n        return `• ${other.name} (${relType})`;\n      }).filter(Boolean);\n\n      return {\n        success: true,\n        answer: `${entity.name} is connected to ${connections.length} ent${connections.length !== 1 ? 'ities' : 'ity'}:\\n\\n${connections.join('\\n')}`\n      };\n    }\n\n    if (type === 'CONTEXT') {\n      // Get entity context\n      let answer = `${entity.name} is a ${entity.type}.`;\n\n      if (entity.context) {\n        answer += `\\n\\n${entity.context}`;\n      }\n\n      const sourceCount = entity.sources?.length || 0;\n      if (sourceCount > 0) {\n        answer += `\\n\\nMentioned in ${sourceCount} article${sourceCount !== 1 ? 's' : ''}.`;\n      }\n\n      return { success: true, answer };\n    }\n\n    return { success: false, error: 'Unknown question type' };\n  } catch (error) {\n    console.error('Answer entity question failed:', error);\n    return { success: false, error: error.message };\n  }\n}\n\nasync function generateAiResearchQuestion(prompt) {\n  try {\n    // Check if Prompt API is available\n    if (typeof LanguageModel === 'undefined' || typeof LanguageModel.create !== 'function') {\n      return {\n        success: false,\n        error: 'Prompt API not available'\n      };\n    }\n\n    // Create AI session for question generation\n    const session = await LanguageModel.create({\n      systemPrompt: 'You are a research assistant that generates focused, searchable research questions. Keep responses concise and specific.'\n    });\n\n    const response = await session.prompt(prompt);\n\n    // Clean up the response\n    let question = response.trim();\n\n    // Remove quotes if present\n    question = question.replace(/^[\"']|[\"']$/g, '');\n\n    // Ensure it doesn't end with a question mark for search queries\n    question = question.replace(/\\?$/, '');\n\n    // Cleanup session\n    session.destroy();\n\n    return {\n      success: true,\n      question: question\n    };\n  } catch (error) {\n    console.error('AI question generation failed:', error);\n    return {\n      success: false,\n      error: error.message\n    };\n  }\n}\n\n// Build context for query\nfunction buildQueryContext(query, graphData) {\n  const queryLower = query.toLowerCase();\n\n  // Find relevant nodes\n  const relevantNodes = graphData.nodes.filter(node =>\n    queryLower.includes(node.name.toLowerCase()) ||\n    node.name.toLowerCase().includes(queryLower)\n  );\n\n  if (relevantNodes.length === 0) {\n    return 'No directly relevant entities found.';\n  }\n\n  // Build context string\n  let context = 'Entities:\\n';\n  for (const node of relevantNodes.slice(0, 5)) {\n    context += `- ${node.name} (${node.type})\\n`;\n\n    // Find connections\n    const connections = graphData.edges?.filter(edge =>\n      edge.source === node.id || edge.target === node.id\n    ) || [];\n\n    if (connections.length > 0) {\n      context += `  Connected to: ${connections.slice(0, 3).map(e => {\n        const otherId = e.source === node.id ? e.target : e.source;\n        const other = graphData.nodes.find(n => n.id === otherId);\n        return other ? `${other.name} (${e.type || 'related'})` : '';\n      }).filter(Boolean).join(', ')}\\n`;\n    }\n  }\n\n  return context;\n}\n\n// Check for duplicate articles before capturing\nasync function checkForDuplicate(data) {\n  try {\n    const { title, text, url } = data;\n    const store = await getGraphStore();\n    const allArticles = await store.getAllArticles();\n\n    if (allArticles.length === 0) {\n      return { success: true, isDuplicate: false, duplicates: [] };\n    }\n\n    // First pass: Check URL similarity (exact match or very similar)\n    const urlDuplicates = allArticles.filter(article => {\n      if (article.url === url) return true;\n\n      // Check if URLs are very similar (same domain + path)\n      try {\n        const url1 = new URL(url);\n        const url2 = new URL(article.url);\n        return url1.hostname === url2.hostname && url1.pathname === url2.pathname;\n      } catch {\n        return false;\n      }\n    });\n\n    if (urlDuplicates.length > 0) {\n      return {\n        success: true,\n        isDuplicate: true,\n        confidence: 100,\n        duplicates: urlDuplicates.map(a => ({\n          id: a.id,\n          title: a.title,\n          url: a.url,\n          capturedAt: a.capturedAt,\n          overlap: 100\n        }))\n      };\n    }\n\n    // Second pass: Title similarity\n    const titleDuplicates = allArticles.filter(article => {\n      const similarity = calculateTitleSimilarity(title, article.title);\n      return similarity > 0.8;\n    });\n\n    if (titleDuplicates.length > 0) {\n      return {\n        success: true,\n        isDuplicate: true,\n        confidence: 90,\n        duplicates: titleDuplicates.map(a => ({\n          id: a.id,\n          title: a.title,\n          url: a.url,\n          capturedAt: a.capturedAt,\n          overlap: Math.round(calculateTitleSimilarity(title, a.title) * 100)\n        }))\n      };\n    }\n\n    // Third pass: Content similarity using summaries (most expensive)\n    const contentDuplicates = await findContentDuplicates(text, allArticles);\n\n    if (contentDuplicates.length > 0) {\n      return {\n        success: true,\n        isDuplicate: true,\n        confidence: 75,\n        duplicates: contentDuplicates\n      };\n    }\n\n    return { success: true, isDuplicate: false, duplicates: [] };\n  } catch (error) {\n    console.error('Duplicate check failed:', error);\n    // Don't block capture if check fails\n    return { success: true, isDuplicate: false, duplicates: [] };\n  }\n}\n\n// Calculate title similarity using word overlap\nfunction calculateTitleSimilarity(title1, title2) {\n  const words1 = new Set(title1.toLowerCase().split(/\\s+/).filter(w => w.length > 3));\n  const words2 = new Set(title2.toLowerCase().split(/\\s+/).filter(w => w.length > 3));\n\n  if (words1.size === 0 || words2.size === 0) return 0;\n\n  const intersection = new Set([...words1].filter(w => words2.has(w)));\n  const union = new Set([...words1, ...words2]);\n\n  return intersection.size / union.size;\n}\n\n// Find content duplicates using entity overlap\nasync function findContentDuplicates(text, allArticles) {\n  try {\n    // Extract entities from new article quickly\n    const newEntities = await extractEntitiesQuick(text);\n\n    if (newEntities.length < 3) {\n      return [];\n    }\n\n    // Get all entities from database\n    const store = await getGraphStore();\n    const dbEntities = await store.getAllEntities();\n    const entityNameToId = new Map();\n    dbEntities.forEach(e => entityNameToId.set(e.name.toLowerCase(), e.id));\n\n    // Match new entities to database\n    const newEntityIds = newEntities\n      .map(name => entityNameToId.get(name.toLowerCase()))\n      .filter(Boolean);\n\n    if (newEntityIds.length < 3) {\n      return [];\n    }\n\n    // Find articles with high entity overlap\n    const duplicates = [];\n    for (const article of allArticles) {\n      const articleEntityIds = article.entities || [];\n      const overlap = articleEntityIds.filter(id => newEntityIds.includes(id)).length;\n      const totalEntities = Math.max(articleEntityIds.length, newEntityIds.length);\n      const overlapPercent = totalEntities > 0 ? (overlap / totalEntities) * 100 : 0;\n\n      if (overlapPercent > 70) {\n        duplicates.push({\n          id: article.id,\n          title: article.title,\n          url: article.url,\n          capturedAt: article.capturedAt,\n          overlap: Math.round(overlapPercent)\n        });\n      }\n    }\n\n    return duplicates;\n  } catch (error) {\n    console.error('Content duplicate detection failed:', error);\n    return [];\n  }\n}\n\n// Analyze page for Glyph Insights (Live Reading Assistant)\nasync function analyzePageForInsights(data) {\n  try {\n    const { content, url, title } = data;\n\n    // Quick entity extraction (simplified for speed)\n    const entities = await extractEntitiesQuick(content);\n\n    if (entities.length === 0) {\n      return { success: true, matches: [], entities: [] };\n    }\n\n    // Find matching articles in database\n    const store = await getGraphStore();\n    const allArticles = await store.getAllArticles();\n    const allEntities = await store.getAllEntities();\n\n    // Build entity name to ID map\n    const entityNameToId = new Map();\n    allEntities.forEach(e => {\n      entityNameToId.set(e.name.toLowerCase(), e.id);\n    });\n\n    // Match extracted entities to database entities\n    const matchedEntityIds = [];\n    const matchedEntityNames = [];\n    for (const entityName of entities) {\n      const entityId = entityNameToId.get(entityName.toLowerCase());\n      if (entityId) {\n        matchedEntityIds.push(entityId);\n        matchedEntityNames.push(entityName);\n      }\n    }\n\n    if (matchedEntityIds.length < 2) {\n      return { success: true, matches: [], entities: matchedEntityNames };\n    }\n\n    // Find articles that contain these entities\n    const articleMatches = [];\n    for (const article of allArticles) {\n      const articleEntityIds = article.entities || [];\n      const sharedEntities = articleEntityIds.filter(id =>\n        matchedEntityIds.includes(id)\n      ).length;\n\n      if (sharedEntities >= 2) {\n        articleMatches.push({\n          id: article.id,\n          title: article.title,\n          url: article.url,\n          capturedAt: article.capturedAt,\n          sharedEntities\n        });\n      }\n    }\n\n    // Sort by shared entities (most relevant first)\n    articleMatches.sort((a, b) => b.sharedEntities - a.sharedEntities);\n\n    // Check for duplicates\n    const duplicateCheck = await checkForDuplicate({ url, title, text: content });\n    const duplicate = duplicateCheck.isDuplicate && duplicateCheck.duplicates.length > 0\n      ? duplicateCheck.duplicates[0]\n      : null;\n\n    // Return top 5 matches + duplicate warning\n    return {\n      success: true,\n      matches: articleMatches.slice(0, 5),\n      entities: matchedEntityNames.slice(0, 5),\n      duplicate: duplicate\n    };\n  } catch (error) {\n    console.error('Page analysis for insights failed:', error);\n    return {\n      success: false,\n      matches: [],\n      entities: [],\n      error: error.message,\n      duplicate: null\n    };\n  }\n}\n\n// Quick entity extraction for live pages (DISABLED FOR TESTING)\nasync function extractEntitiesQuick(text) {\n  console.log('[DEBUG] 🚀 EXTRACTENTITIESQUICK DISABLED - Using fallback');\n  // Skip AI entirely for testing\n  return extractKeywordsSimple(text);\n}\n\n// Simple keyword extraction fallback\nfunction extractKeywordsSimple(text) {\n  // Extract capitalized phrases (potential entities)\n  const words = text.split(/\\s+/);\n  const entities = new Set();\n\n  for (let i = 0; i < words.length && entities.size < 8; i++) {\n    const word = words[i];\n    // Check if starts with capital letter and is not common word\n    if (/^[A-Z][a-z]+/.test(word) && word.length > 3) {\n      entities.add(word);\n    }\n    // Check for multi-word capitalized phrases\n    if (i < words.length - 1) {\n      const nextWord = words[i + 1];\n      if (/^[A-Z][a-z]+/.test(word) && /^[A-Z][a-z]+/.test(nextWord)) {\n        entities.add(`${word} ${nextWord}`);\n      }\n    }\n  }\n\n  return Array.from(entities).slice(0, 8);\n}\n\n// Open graph page with article selected\nasync function openGraphPage(articleId) {\n  try {\n    const url = chrome.runtime.getURL('graph-page/index.html');\n    const fullUrl = articleId ? `${url}?article=${articleId}` : url;\n\n    await chrome.tabs.create({ url: fullUrl });\n\n    return { success: true };\n  } catch (error) {\n    console.error('Failed to open graph page:', error);\n    return { success: false, error: error.message };\n  }\n}\n\n// Test Writer API quality with sample or real data\nasync function testWriterAPI() {\n  try {\n    const store = await getGraphStore();\n\n    // Try to get real articles from last 7 days\n    const allArticles = await store.getAllArticles();\n    const sevenDaysAgo = Date.now() - (7 * 24 * 60 * 60 * 1000);\n    const recentArticles = allArticles.filter(a => a.capturedAt > sevenDaysAgo);\n\n    let testData;\n    if (recentArticles.length >= 3) {\n      // Use real data\n      const allEntities = await store.getAllEntities();\n\n      // Count entity frequencies from recent articles\n      const entityFrequency = new Map();\n      for (const article of recentArticles) {\n        for (const entityId of article.entities || []) {\n          entityFrequency.set(entityId, (entityFrequency.get(entityId) || 0) + 1);\n        }\n      }\n\n      // Get top 15 entities\n      const topEntities = Array.from(entityFrequency.entries())\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 15)\n        .map(([id, freq]) => {\n          const entity = allEntities.find(e => e.id === id);\n          return entity ? { name: entity.name, frequency: freq } : null;\n        })\n        .filter(Boolean);\n\n      testData = {\n        articles: recentArticles.map(a => ({\n          title: a.title,\n          capturedAt: a.capturedAt\n        })),\n        topEntities\n      };\n    } else {\n      // Use sample data\n      testData = generateSampleData();\n    }\n\n    // Test Writer API\n    const result = await testWriterQuality(testData.articles, testData.topEntities);\n\n    return {\n      success: true,\n      ...result,\n      dataSource: recentArticles.length >= 3 ? 'real' : 'sample',\n      articlesCount: testData.articles.length\n    };\n  } catch (error) {\n    console.error('Writer API test failed:', error);\n    return {\n      success: false,\n      quality: 'error',\n      error: error.message,\n      recommendation: 'Skip - API test failed'\n    };\n  }\n}\n\n// Synthesize weekly learning using Writer API\nasync function synthesizeWeek(weekStart = null, force = false) {\n  try {\n    // Check Writer API availability\n    if (typeof Writer === 'undefined' || typeof Writer.create !== 'function') {\n      return {\n        success: false,\n        error: 'Writer API not available in this Chrome version. Update to Chrome 142+ or enable experimental flags.'\n      };\n    }\n\n    try {\n      const availability = await Writer.availability();\n      if (availability !== 'readily' && availability !== 'available') {\n        return {\n          success: false,\n          error: `Writer API not ready. Status: ${availability}`\n        };\n      }\n    } catch (error) {\n      return {\n        success: false,\n        error: `Writer API check failed: ${error.message}`\n      };\n    }\n\n    const store = await getGraphStore();\n\n    // Get ALL articles (no week filtering)\n    const allArticles = await store.getAllArticles();\n\n    console.log(`Found ${allArticles.length} total articles:`, allArticles.map(a => a.title));\n\n    if (allArticles.length === 0) {\n      return {\n        success: false,\n        error: 'No articles captured yet'\n      };\n    }\n\n    // Get all entities\n    const allEntities = await store.getAllEntities();\n\n    // Count entity frequencies from all articles\n    const entityFrequency = new Map();\n    for (const article of allArticles) {\n      for (const entityId of article.entities || []) {\n        entityFrequency.set(entityId, (entityFrequency.get(entityId) || 0) + 1);\n      }\n    }\n\n    // Get top 15 entities\n    const topEntities = Array.from(entityFrequency.entries())\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, 15)\n      .map(([id, freq]) => {\n        const entity = allEntities.find(e => e.id === id);\n        return entity ? { name: entity.name, type: entity.type, frequency: freq } : null;\n      })\n      .filter(Boolean);\n\n    if (topEntities.length === 0) {\n      return {\n        success: false,\n        error: 'No entities found in recent articles'\n      };\n    }\n\n    // Create Writer session with specific context\n    const writer = await Writer.create({\n      sharedContext: `You are a world-class learning analyst who crafts insightful narratives about people's learning journeys. You synthesize information into compelling, coherent stories that reveal patterns and connections.`\n    });\n\n    // Build prompt with article and entity data\n    const entityList = topEntities\n      .map(e => `- ${e.name} (${e.type}, appeared ${e.frequency} times)`)\n      .join('\\n');\n\n    const articleTitles = allArticles\n      .slice(0, 10)\n      .map(a => `- \"${a.title}\"`)\n      .join('\\n');\n\n    const prompt = `Based on the following learning data, write a 150-200 word narrative synthesis that tells the story of this person's intellectual journey. Focus on themes, connections, and insights rather than just listing topics.\n\nArticles Read (${allArticles.length} total):\n${articleTitles}\n\nMost Frequent Entities:\n${entityList}\n\nWrite a narrative that:\n1. Identifies 2-3 major themes or topics\n2. Highlights interesting connections between entities\n3. Suggests what the person is learning about or interested in\n4. Uses a warm, insightful tone\n\nSynthesis:`;\n\n    const synthesis = await writer.write(prompt, {\n      context: 'This is a personal learning summary for educational purposes.'\n    });\n\n    writer.destroy();\n\n    // Store synthesis in database with timestamp\n    const now = Date.now();\n    const synthesisId = `synthesis_${now}`;\n\n    const synthesisRecord = {\n      id: synthesisId,\n      synthesis: synthesis.trim(),\n      createdAt: now,\n      weekStart: now, // Keep for UI compatibility but just use current time\n      articlesAnalyzed: allArticles.length,\n      topEntities: topEntities.slice(0, 5).map(e => e.name)\n    };\n\n    await store.db.add('syntheses', synthesisRecord);\n\n    return {\n      success: true,\n      synthesis: synthesis.trim(),\n      articlesAnalyzed: allArticles.length,\n      topEntities: topEntities.slice(0, 5).map(e => e.name),\n      createdAt: now,\n      weekStart: now\n    };\n  } catch (error) {\n    console.error('Weekly synthesis failed:', error);\n    return {\n      success: false,\n      error: error.message || 'Unknown error during synthesis'\n    };\n  }\n}\n\nasync function synthesizeWeekBackground(weekStart, force = false) {\n  try {\n    const result = await synthesizeWeek(weekStart, force);\n    if (result.success) {\n      console.log('Weekly synthesis completed in background');\n      chrome.runtime.sendMessage({\n        type: 'SYNTHESIS_COMPLETE',\n        data: result\n      }).catch(() => {});\n    }\n  } catch (error) {\n    console.error('Background synthesis failed:', error);\n  }\n}\n\nasync function getAllSyntheses() {\n  try {\n    const store = await getGraphStore();\n    const allSyntheses = await store.db.getAll('syntheses');\n\n    allSyntheses.sort((a, b) => b.createdAt - a.createdAt);\n\n    return {\n      success: true,\n      syntheses: allSyntheses\n    };\n  } catch (error) {\n    console.error('Failed to get syntheses:', error);\n    return {\n      success: false,\n      error: error.message\n    };\n  }\n}\n\nasync function getLatestSynthesis() {\n  try {\n    const result = await getAllSyntheses();\n    if (result.success && result.syntheses.length > 0) {\n      return {\n        success: true,\n        synthesis: result.syntheses[0]\n      };\n    }\n    return {\n      success: false,\n      error: 'No syntheses found'\n    };\n  } catch (error) {\n    console.error('Failed to get latest synthesis:', error);\n    return {\n      success: false,\n      error: error.message\n    };\n  }\n}\n\n// Add these to the message handler\n// Update handleMessage function to include these new cases:\n/*\ncase 'QUERY_GRAPH':\n  return await queryGraph(message.query, message.graphData);\n\n\ncase 'CAPTURE_ALL_TABS':\n  return await captureAllOpenTabs();\n*/\n"],"names":[],"sourceRoot":""}